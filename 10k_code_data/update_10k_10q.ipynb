{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "d6b43054",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\deepali.b\\Anaconda3\\envs\\tf\\lib\\site-packages\\tqdm\\auto.py:22: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "# Import the required library\n",
    "from bs4 import BeautifulSoup\n",
    "from collections import Counter\n",
    "from collections import OrderedDict\n",
    "import spacy\n",
    "from string import punctuation\n",
    "import collections\n",
    "import numpy as np\n",
    "import pickle\n",
    "from sec_edgar_downloader import Downloader\n",
    "import json\n",
    "import bs4\n",
    "import bs4 as bs\n",
    "import requests\n",
    "import re\n",
    "import time\n",
    "import datetime\n",
    "from datetime import date\n",
    "from dateutil.relativedelta import relativedelta\n",
    "from sec_api import QueryApi\n",
    "from sec_api import ExtractorApi # https://pypi.org/project/sec-api/\n",
    "import matplotlib.pyplot as plot\n",
    "import pytextrank\n",
    "from fnmatch import fnmatch\n",
    "import os\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import shutil\n",
    "import unicodedata\n",
    "import locale\n",
    "locale.setlocale(locale.LC_ALL, 'en_US.UTF-8')\n",
    "'en_US.UTF-8'\n",
    "from pandas.api.types import is_string_dtype\n",
    "from pandas.api.types import is_numeric_dtype\n",
    "import itertools\n",
    "from time import sleep\n",
    "from apscheduler.schedulers.background import BackgroundScheduler, BlockingScheduler\n",
    "from apscheduler.triggers.cron import CronTrigger\n",
    "import schedule"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "id": "f6e89a67",
   "metadata": {},
   "outputs": [],
   "source": [
    "global DIR_PATH\n",
    "DIR_PATH = \"C:\\\\Users\\\\deepali.b\\\\DL_tensorflow\\\\10k_document\\\\\"\n",
    "#company_ticker = str(input(\"Please enter the company ticker\"))\n",
    "year = date.today().year\n",
    "form_type_q = '10-Q'\n",
    "form_type_k = '10-K'\n",
    "company_ticker = ['MDT','STE','SYK','JNJ','GMED']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "4cc7bd70",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input path to save files: D:\\\\work\\\\10kform\\\\10k_download\n",
      "Input year: 2022\n",
      "Enter the number of companies: 2\n",
      "\n",
      "\n",
      "Enter company ticker separated by space MDT STE\n",
      "Input form type: 10-Q\n",
      "Input number to get range: 2\n"
     ]
    }
   ],
   "source": [
    "year = int(input(\"Input year: \"))\n",
    "n = int(input(\"Enter the number of companies: \"))\n",
    "print(\"\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "15a40435",
   "metadata": {},
   "outputs": [],
   "source": [
    "year_list = [year-i for i in range(output_range)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "9d4d7d90",
   "metadata": {},
   "outputs": [],
   "source": [
    "# scheduling job to run function everyday\n",
    "scheduler = BackgroundScheduler()\n",
    "scheduler.start()\n",
    "trigger = CronTrigger(\n",
    "    year=\"*\", month=\"*\", day=\"*\", hour=\"5\", minute=\"0\", second=\"5\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "caca2fb5",
   "metadata": {},
   "outputs": [],
   "source": [
    "def crete_temp_folder(folder_path):\n",
    "    #folder_path = '10k_download/'\n",
    "    if os.path.exists(folder_path):\n",
    "        shutil.rmtree(folder_path)\n",
    "        #os.makedirs(newpath)\n",
    "    else:\n",
    "        os.makedirs(folder_path)\n",
    "    return folder_path"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7819abc1",
   "metadata": {},
   "source": [
    "#### 10q download and update"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "a933a66b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_time_update():\n",
    "    print('DIR_PATH')\n",
    "    print(DIR_PATH)\n",
    "    with open(DIR_PATH+'\\\\time_update.pkl', 'rb') as f:\n",
    "        time_updated_data = pickle.load(f)\n",
    "    time_updated_data[\"STE\"].iloc[0] = '30-08-2022'\n",
    "    time_update = list(time_updated_data.iloc[0].values)\n",
    "    date_format = [datetime.datetime.strptime(elem, '%d-%m-%Y') for elem in time_update]\n",
    "    now = datetime.datetime.now()\n",
    "    time_diff = [(now-elem).days for elem in date_format]\n",
    "    time_updated_data.loc[len(time_updated_data.index)] = time_diff\n",
    "    time_diff1 = time_updated_data.iloc[1]\n",
    "    time_diff1s = time_diff1.gt(95)\n",
    "    updated_company = list(time_diff1s[time_diff1s].index.values)\n",
    "    return updated_company"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "b5559599",
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_fin_elem(file_location,company_ticker,file,form_type):\n",
    "    '''This function takes 4 inputs as mentioned in arguments and returns the list of \n",
    "    Net sales and Cost of service through regex pattern'''\n",
    "    #create empty list to append net sales and cost of service values\n",
    "    filter_net_sales = []\n",
    "    filter_cost_service = []\n",
    "    #create empty list to append values with their respective quarters\n",
    "    quarter_list =[]\n",
    "    #path for search the text file of 10-Q report\n",
    "    path = file_location + \"sec-edgar-filings\\\\\"+company_ticker +\"\\\\\"+form_type+\"\\\\\"+ file\n",
    "    os.chdir(path)\n",
    "    #open text file\n",
    "    f = open('full-submission.txt', 'r')\n",
    "    #Read text file\n",
    "    content = f.read()\n",
    "    #word = 'CONFORMED PERIOD OF REPORT:'\n",
    "    #regex pattern for extract report date\n",
    "    date_reg = re.findall(r'(?P<name>(CONFORMED PERIOD OF REPORT:)[\\W\\d]*)',content)[0][0]\n",
    "    dtreg_lst = date_reg.split()\n",
    "    for sub1 in dtreg_lst:\n",
    "        reg12 = re.sub('[^0-9]+', '', sub1)\n",
    "        if reg12:\n",
    "            year_date = reg12\n",
    "    #we only need month from report date.\n",
    "    month = year_date[4:6]\n",
    "    #print(year_end)\n",
    "    # Regex to find <DOCUMENT> tags\n",
    "    doc_start_pattern = re.compile(r'<DOCUMENT>')\n",
    "    doc_end_pattern = re.compile(r'</DOCUMENT>')\n",
    "    # Regex to find <TYPE> tag prceeding any characters, terminating at new line\n",
    "    type_pattern = re.compile(r'<TYPE>[^\\n]+')\n",
    "    # Create 3 lists with the span indices for each regex\n",
    "    ### There are many <Document> Tags in this text file, each as specific exhibit like 10-K, EX-10.17 etc\n",
    "    ### First filter will give us document tag start <end> and document tag end's <start> \n",
    "    ### We will use this to later grab content in between these tags\n",
    "    doc_start_is = [x.end() for x in doc_start_pattern.finditer(content)]\n",
    "    doc_end_is = [x.start() for x in doc_end_pattern.finditer(content)]\n",
    "\n",
    "    ### Type filter is interesting, it looks for <TYPE> with Not flag as new line, ie terminare there, with + sign\n",
    "    ### to look for any char afterwards until new line \\n. This will give us <TYPE> followed Section Name like '10-K'\n",
    "    ### Once we have have this, it returns String Array, below line will with find content after <TYPE> ie, '10-K' \n",
    "    ### as section names\n",
    "    doc_types = [x[len('<TYPE>'):] for x in type_pattern.findall(content)]\n",
    "    # Create a loop to go through each section type and save only the 10-K section in the dictionary\n",
    "    document = {}\n",
    "\n",
    "    for doc_type, doc_start, doc_end in zip(doc_types, doc_start_is, doc_end_is):\n",
    "        if doc_type == form_type:\n",
    "            document[doc_type] = content[doc_start:doc_end]\n",
    "    #Print 10-Q document dict\n",
    "#     print(document[form_type][0:1000])\n",
    "    \n",
    "    #Regex pattern for extract statements of income\n",
    "    regex = re.compile(r'(Consolidated Statements of Income)|(CONSOLIDATED STATEMENTS OF INCOME)|(CONSOLIDATED STATEMENTS OF EARNINGS)|(CONSOLIDATED STATEMENTS OF OPERATIONS AND COMPREHENSIVE INCOME)|(CONSOLIDATED STATEMENTS OF OPERATIONS)')\n",
    "    # Use finditer to math the regex\n",
    "    matches = regex.finditer(document[form_type])\n",
    "    # Write a for loop to print the matches\n",
    "#     for match in matches:\n",
    "#         print(match)\n",
    "\n",
    "    # Matches\n",
    "    matches = regex.finditer(document[form_type])\n",
    "    # Create the dataframe\n",
    "    test_df = pd.DataFrame([(x.group(), x.start(), x.end()) for x in matches])\n",
    "    test_df.columns = ['item', 'start', 'end']\n",
    "    # Display the dataframe\n",
    "    test_df.head()\n",
    "    \n",
    "    #Replacing the end value to none\n",
    "    test_df['end'] = test_df['end'].replace('','',regex=True,inplace=True)\n",
    "    #Replacing the financial metric name to parameter\n",
    "    test_df.loc[0,\"item\"] = \"Parameter\"\n",
    "    test_df.head()\n",
    "    \n",
    "    # Drop duplicates\n",
    "    pos_dat = test_df.sort_values('start', ascending=True).drop_duplicates(subset=['item'])\n",
    "    # Display the dataframe\n",
    "    pos_dat\n",
    "    \n",
    "    # Set item as the dataframe index\n",
    "    pos_dat.set_index('item', inplace=True)\n",
    "    # display the dataframe\n",
    "    pos_dat\n",
    "    \n",
    "    #Get item 8\n",
    "    item_8_raw = document[form_type][pos_dat['start'].loc['Parameter']:pos_dat['end'].loc['Parameter']]\n",
    "    \n",
    "    # First convert the raw text we have to exrtacted to BeautifulSoup object \n",
    "    Fin_8A = BeautifulSoup(item_8_raw, 'lxml')\n",
    "    \n",
    "    # Finding out metrices using tags\n",
    "    Fin_headings=[]\n",
    "    for i in Fin_8A.findAll('table'):\n",
    "        Fin_headings.append(i.text)\n",
    "    for i in Fin_8A.findAll('span'):\n",
    "        Fin_headings.append(i.text)\n",
    "    for i in Fin_8A.findAll('td'):\n",
    "        Fin_headings.append(i.text)\n",
    "    Fin_headings = ''.join(Fin_headings)\n",
    "    Fin_text = unicodedata.normalize(\"NFKD\",Fin_headings)\n",
    "    \n",
    "    ###Process of extract Net sales and cost of service through regex pattern\n",
    "    try:\n",
    "        Reg = re.findall(r'(?P<name>(Net.sales|Sales.to.customers|Net.Sales|Sales)[\\W\\d]*)',Fin_text) #MDT SYK \n",
    "#         Reg = re.findall(r'(?P<name>(Sales\\sto\\scustomers\\s\\((.*?)\\])[\\W\\d]*)',Fin_text) #JNJ \n",
    "        net_sales = Reg[0][0]   \n",
    "    except:\n",
    "        pass\n",
    "    try:\n",
    "        Reg = re.findall(r'(?P<name>(Total.revenues)[\\W\\d]*)',Fin_text) #MDT SYK \n",
    "        net_sales = Reg[-1][0]\n",
    "    except:\n",
    "        pass\n",
    "    try:\n",
    "        Reg = re.findall(r'(?P<name>(Cost.of.products.sold,.excluding.amortization.of.intangible.assets|Cost.of.products.sold|Cost.of.goods.sold|Cost.of.sales|Total.cost.of.revenues)[\\W\\d]*)',Fin_text) #MDT SYK \n",
    "        cost_of_service = Reg[0][0]\n",
    "    except:\n",
    "        pass\n",
    "    #split the whole regex string to identify only useful string\n",
    "    sales_lst = net_sales.split()\n",
    "    for char in sales_lst:\n",
    "    #\\$[0-9]+   [^$0-9]+\n",
    "    #regex pattern to extract only numbers(i.e. net sales)\n",
    "        reg1 = re.sub('[^0-9]+', '', char)\n",
    "        if reg1:\n",
    "            #Append the needed string to new list\n",
    "            filter_net_sales.append(reg1)\n",
    "            break\n",
    "        else:\n",
    "            continue\n",
    "    #split the whole regex string to identify only useful string\n",
    "    service_lst = cost_of_service.split()\n",
    "    for elem in service_lst:\n",
    "        #regex pattern to extract only numbers(i.e. cost of service)\n",
    "        reg2 = re.sub('[^$0-9]+', '', elem)\n",
    "        if reg2:\n",
    "            #Append the needed string to new list\n",
    "            filter_cost_service.append(reg2)\n",
    "            break\n",
    "        else:\n",
    "            continue\n",
    "    \n",
    "\n",
    "    #For the company- steris and GMED the values will be divide by 1000\n",
    "    if (company_ticker == 'STE' or company_ticker == 'GMED'):\n",
    "        for i,x in enumerate(filter_net_sales):\n",
    "            filter_values = round(int(x)/1000)\n",
    "            filter_net_sales[i] = filter_values\n",
    "        for j,y in enumerate(filter_cost_service):\n",
    "            fil_value = round(int(y)/1000)\n",
    "            filter_cost_service[i] = fil_value\n",
    "    #Here we create empty list for append the company ticker with special symbol.\n",
    "    company_lst = []\n",
    "    if form_type == '10-Q':\n",
    "        comp = company_ticker+\"(M$)\"\n",
    "    elif form_type == '10-K':\n",
    "        comp = company_ticker+\"(M$)\"\n",
    "    company_lst.append(comp)\n",
    "#     quarter_list = list(OrderedDict.fromkeys(quarter_list))\n",
    "    #Create list of list with the parameters of company name and it's respected financial value \n",
    "    net_sales_list = list(zip(company_lst,filter_net_sales))\n",
    "    cost_service_list = list(zip(company_lst,filter_cost_service))\n",
    "    return net_sales_list,cost_service_list\n",
    "#%%%"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "cb015b1e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def calling_function(file_location,company_list,form_type):\n",
    "    '''This function takes the file location and company list as a input and returns \n",
    "    4 dataframe of 4 financial parameters.(i.e.- Net sales,Cost of service,Gross margin,Revenue growth)'''\n",
    "    #Create empty list of sales,cost of service\n",
    "    sales_list = []\n",
    "    service_list = []\n",
    "    company_list = [\"MDT\", \"STE\", \"SYK\", \"JNJ\", \"GMED\"]\n",
    "    #iterate through all companies to extract financial parameters for all companies \n",
    "    for company in company_list:\n",
    "        path = file_location +\"\\\\sec-edgar-filings\\\\\"+ company +\"\\\\\"+form_type\n",
    "        file_list = os.listdir(path)\n",
    "        for file in file_list:\n",
    "            print('file')\n",
    "            print(file)\n",
    "            #calling the extract_fin_elem function\n",
    "            net_sales_list,cost_service_list = extract_fin_elem(file_location,company,file,form_type)\n",
    "            #Extend the list in parent list.\n",
    "            sales_list.extend(net_sales_list)\n",
    "            service_list.extend(cost_service_list)       \n",
    "    #create dataframe for net sales  \n",
    "    sales_df = pd.DataFrame(sales_list, columns=['Company', 'Net sales values(M$)'])\n",
    "#     print(sales_df)\n",
    "    #create list of all values of Company column\n",
    "    company_values = sales_df['Company'].tolist()\n",
    "    #Transpose the dataframe to fulfilled the requirenment \n",
    "    sales_trans_df = sales_df.set_index('Company').T\n",
    "    #create dataframe for cost of service\n",
    "    service_df = pd.DataFrame(service_list, columns=['Company', 'Cost of service values(M$)'])\n",
    "    #Transpose the dataframe to fulfilled the requirenment \n",
    "    cost_trans_df = service_df.set_index('Company').T\n",
    "    #To calculate the gross margin we need cost of service value and net sales values so we merged both dataframe\n",
    "    combined_df = pd.merge(sales_df,service_df,on='Company')\n",
    "    #Apply function to calculate gross margin\n",
    "    combined_df['Gross margin(%)'] = combined_df.apply(lambda x: calc_gross_margin(x['Net sales values(M$)'], x['Cost of service values(M$)']), axis=1)\n",
    "    #Change the column values through lambda expression.\n",
    "    combined_df['Company']=combined_df['Company'].apply(lambda x: x.replace('(M$)','(%)') if x.endswith('(M$)') else x)\n",
    "    #We only need Company and gross margin in this dataframe so we created dataframe with these two columns\n",
    "    combined_df = combined_df[['Company','Gross margin(%)']]\n",
    "    #Transpose the dataframe to fulfilled the requirenment \n",
    "    gross_margin_trans_df = combined_df.set_index('Company').T\n",
    "    #load the existing pickle file for the use of net sales dataframe\n",
    "#     print(form_type)\n",
    "    if form_type == '10-Q':\n",
    "        with open('net_sales_10q.pkl', 'rb') as f:\n",
    "            net_sales_10q = pickle.load(f)\n",
    "        #revenue growth function calling\n",
    "        revenue_growth_values = calc_revenue_growth(net_sales_10q,sales_df,company_values)\n",
    "    elif form_type == '10-K':\n",
    "#         print('YES-10-K')\n",
    "        with open('net_sales.pkl', 'rb') as f:\n",
    "            net_sales = pickle.load(f)\n",
    "        #revenue growth function calling\n",
    "        revenue_growth_values = calc_revenue_growth(net_sales,sales_df,company_values)\n",
    "    #Create the list of list for prepare the revenue growth dataframe\n",
    "    revenue_growth_lst = list(zip(company_values,revenue_growth_values))\n",
    "    #Revenue growth dataframe\n",
    "    growth_df = pd.DataFrame(revenue_growth_lst, columns=['Company', 'Revenue growth(%)'])\n",
    "    #Change the column values through lambda expression.\n",
    "    growth_df['Company']=growth_df['Company'].apply(lambda x: x.replace('(M$)','(%)') if x.endswith('(M$)') else x)\n",
    "    #Transpose the dataframe to fulfilled the requirenment \n",
    "    rev_growth_df = growth_df.set_index('Company').T\n",
    "    return sales_trans_df,cost_trans_df,gross_margin_trans_df,rev_growth_df\n",
    "#%%%"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "05da6384",
   "metadata": {},
   "outputs": [],
   "source": [
    "def calc_revenue_growth(net_sales,sales_df,company_list):\n",
    "    '''This function takes the current net sales values(i.e.-from sales_df) and\n",
    "    previous net sales values(i.e.-from net_sales_10q) and company list as a \n",
    "    input and return the revenue growth of all companies as a list'''\n",
    "    revenue_growth_values = []\n",
    "    print('sales_df in revenue')\n",
    "    print(sales_df)\n",
    "    for company in company_list:\n",
    "        revenue_growth = round((int(sales_df.loc[sales_df['Company'] == company, 'Net sales values(M$)'].iloc[0]) - int(locale.atoi(net_sales.loc[net_sales.index[0], company])))/int(locale.atoi(net_sales.loc[net_sales.index[0], company]))*100,2)\n",
    "        revenue_growth_values.append(revenue_growth)\n",
    "    return revenue_growth_values\n",
    "#%%%"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "83c81f96",
   "metadata": {},
   "outputs": [],
   "source": [
    "def calc_gross_margin(net_sales,cost_service):\n",
    "    '''This function takes the net sales & cost of service values as a input and return \n",
    "    the gross margin of each row. Row wise one by one calculate for all the values'''\n",
    "    gross_margin = round((int(net_sales) - int(cost_service))/int(net_sales)*100,2)\n",
    "    return gross_margin\n",
    "#%%%"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "dd5817c2",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_quarter(last_q):\n",
    "    if len(get_time_update())>0:\n",
    "        # extract from code and update in pickle also update time pickle\n",
    "        #last_q = net_sales_10q['Quarters/ USD millions'][0]\n",
    "        print('last_q')\n",
    "        print(last_q)\n",
    "        quarter = last_q.split(\"-\")[0]\n",
    "        print('quarter')\n",
    "        print(quarter)\n",
    "        year = int(last_q.split(\"-\")[1])\n",
    "        if quarter!='Q4 ':\n",
    "            new_q = int([re.findall(r'(\\w+?)(\\d+)', last_q.split(\"-\")[0])[0]][0][1])+1\n",
    "            new_quarter = 'Q' + str(new_q)\n",
    "            new_quarter_year = new_quarter + ' - ' + str(year)\n",
    "        else:\n",
    "            new_quarter_year = 'Q1 ' + ' - ' + str(year+1)\n",
    "    print('new_quarter')\n",
    "    print(new_quarter_year)\n",
    "    return new_quarter_year"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "ce84cb10",
   "metadata": {},
   "outputs": [],
   "source": [
    "def download_files(file_location,company_ticker, form_type):\n",
    "    try:\n",
    "#         if os.path.exists(file_location):\n",
    "#             shutil.rmtree(file_location)\n",
    "        dl = Downloader(file_location)\n",
    "        dl.get(form_type,company_ticker,amount=1)\n",
    "        print('Download report successfully')\n",
    "        print(file_location)\n",
    "    except Exception as e:\n",
    "        print(e)\n",
    "        print(\"Error in downloading file\")\n",
    "    return file_location"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "50a36157",
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_file_date(file_location,company_ticker):\n",
    "    \n",
    "    '''This function returns the form type with their file date'''\n",
    "    #create empty list to store form type with file date\n",
    "    form_lst_with_date = {}\n",
    "    base_getcwd = DIR_PATH\n",
    "    print('base_getcwd')\n",
    "    print(base_getcwd)\n",
    "    #extract forms types for every company at given path\n",
    "    form_list = os.listdir(base_getcwd +file_location+'\\\\sec-edgar-filings\\\\'+company_ticker)\n",
    "    for form in form_list:\n",
    "        if(form =='10-K'):\n",
    "            form = '10-K'\n",
    "        else:\n",
    "            form = '10-Q'\n",
    "        #go to the form type location and extract all files present in that folder \n",
    "        file_list = os.listdir(base_getcwd+file_location+'\\\\sec-edgar-filings\\\\'+company_ticker+'\\\\'+form)\n",
    "        base_path = base_getcwd +file_location+'\\\\sec-edgar-filings\\\\'+company_ticker+'\\\\'+form\n",
    "        #for loop to iterate over every file present in that folder\n",
    "        for file in file_list:\n",
    "            path = base_path+'\\\\'+file\n",
    "            #reach to the text file\n",
    "            os.chdir(path)\n",
    "            #open text file\n",
    "            f = open('full-submission.txt', 'r')\n",
    "            #read the content of text file\n",
    "            content = f.read()\n",
    "            #create regex pattern for extract filing date\n",
    "            date_reg = re.findall(r'(?P<name>(FILED AS OF DATE:)[\\W\\d]*)',content)[0][0]\n",
    "            #this variable contain file date with some unnecessary words, so split them for create list\n",
    "            dtreg_lst = date_reg.split()\n",
    "            #iterate through every words to find the our needed file date.\n",
    "            for sub1 in dtreg_lst:\n",
    "                #create regex pattern to store only file date.\n",
    "                reg12 = re.sub('[^0-9]+', '', sub1)\n",
    "                if reg12:\n",
    "                    year_date = reg12\n",
    "            #convert that string date to date time object\n",
    "            date_object = datetime.datetime.strptime(year_date, '%Y%m%d').strftime('%Y-%m-%d')\n",
    "            filed_date = date_object\n",
    "            form_lst_with_date.update({form:filed_date})\n",
    "#             form_lst_with_date.append(filed_date)\n",
    "    return form_lst_with_date\n",
    "#%%"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "ce1ab0cd",
   "metadata": {},
   "outputs": [],
   "source": [
    "def calling_function_10k(file_location,company_list,form_type):\n",
    "    '''This function takes the file location and company list as a input and returns \n",
    "    4 dataframe of 4 financial parameters.(i.e.- Net sales,Cost of service,Gross margin,Revenue growth)'''\n",
    "    #Create empty list of sales,cost of service\n",
    "    sales_list = []\n",
    "    service_list = []\n",
    "    #iterate through all companies to extract financial parameters for all companies \n",
    "    for company in company_list:\n",
    "        path = file_location+\"\\\\sec-edgar-filings\\\\\" + company +\"\\\\\"+form_type\n",
    "        file_list = os.listdir(path)\n",
    "        for file in file_list:\n",
    "            #calling the extract_fin_elem function\n",
    "            net_sales_list,cost_service_list = extract_fin_elem(file_location,company,file,form_type)\n",
    "            #Extend the list in parent list.\n",
    "            sales_list.extend(net_sales_list)\n",
    "            service_list.extend(cost_service_list)       \n",
    "    #create dataframe for net sales  \n",
    "    sales_df = pd.DataFrame(sales_list, columns=['Company', 'Net sales values(M$)'])\n",
    "    print('sales_df')\n",
    "    print(sales_df)\n",
    "    #create list of all values of Company column\n",
    "    company_values = sales_df['Company'].tolist()\n",
    "    #Transpose the dataframe to fulfilled the requirenment \n",
    "    sales_trans_df = sales_df.set_index('Company').T\n",
    "    #create dataframe for cost of service\n",
    "    service_df = pd.DataFrame(service_list, columns=['Company', 'Cost of service values(M$)'])\n",
    "    #Transpose the dataframe to fulfilled the requirenment \n",
    "    cost_trans_df = service_df.set_index('Company').T\n",
    "    #load the existing pickle file for the use of net sales dataframe\n",
    "#     print(form_type)\n",
    "    pckl_file_path = DIR_PATH\n",
    "    with open(pckl_file_path + 'net_sales_10q.pkl', 'rb') as f:\n",
    "        net_sales_10q = pickle.load(f)\n",
    "    with open(pckl_file_path + 'cost_of_service_10q.pkl', 'rb') as f:\n",
    "        cost_of_service_10q = pickle.load(f)\n",
    "    with open(pckl_file_path + 'net_sales.pkl', 'rb') as f:\n",
    "            net_sales = pickle.load(f)\n",
    "#     with open(pckl_file_path + 'cost_of_products.pkl', 'rb') as f:\n",
    "#             cost_of_products.pkl = pickle.load(f)\n",
    "    q4_sales_lst,q4_cost_lst = q4_calculation(net_sales_10q,cost_of_service_10q,sales_df,service_df,company_values)\n",
    "    q4_sales_lst_zip = list(zip(company_values,q4_sales_lst))\n",
    "    q4_cost_lst_zip = list(zip(company_values,q4_cost_lst))\n",
    "    q4_sales_df = pd.DataFrame(q4_sales_lst_zip, columns=['Company', 'Net sales(M$)'])\n",
    "    q4_cost_df = pd.DataFrame(q4_cost_lst_zip, columns=['Company', 'Cost Of Service(M$)'])\n",
    "    q4_sales_trans_df = q4_sales_df.set_index('Company').T\n",
    "    q4_cost_trans_df = q4_cost_df.set_index('Company').T\n",
    "    \n",
    "    #To calculate the gross margin we need cost of service value and net sales values so we merged both dataframe\n",
    "    combined_df = pd.merge(q4_sales_df,q4_cost_df,on='Company')\n",
    "    #Apply function to calculate gross margin\n",
    "    combined_df['Gross margin(%)'] = combined_df.apply(lambda x: calc_gross_margin(x[combined_df.columns.values.tolist()[1]], x[combined_df.columns.values.tolist()[2]]), axis=1)\n",
    "    #Change the column values through lambda expression.\n",
    "    combined_df['Company']=combined_df['Company'].apply(lambda x: x.replace('(M$)','(%)') if x.endswith('(M$)') else x)\n",
    "    #We only need Company and gross margin in this dataframe so we created dataframe with these two columns\n",
    "    combined_df = combined_df[['Company','Gross margin(%)']]\n",
    "    #Transpose the dataframe to fulfilled the requirenment \n",
    "    gross_margin_trans_df = combined_df.set_index('Company').T\n",
    "    company_values = q4_sales_df['Company'].tolist()\n",
    "    if form_type == '10-Q':\n",
    "        #revenue growth function calling\n",
    "        revenue_growth_values = calc_revenue_growth(net_sales_10q,sales_df,company_values)\n",
    "    elif form_type == '10-K':\n",
    "        #revenue growth function calling\n",
    "        revenue_growth_values = calc_revenue_growth_q4(net_sales_10q,q4_sales_df,company_values)\n",
    "    #Create the list of list for prepare the revenue growth dataframe\n",
    "    revenue_growth_lst = list(zip(company_values,revenue_growth_values))\n",
    "    #Revenue growth dataframe\n",
    "    growth_df = pd.DataFrame(revenue_growth_lst, columns=['Company', 'Revenue growth(%)'])\n",
    "    #Change the column values through lambda expression.\n",
    "    growth_df['Company']=growth_df['Company'].apply(lambda x: x.replace('(M$)','(%)') if x.endswith('(M$)') else x)\n",
    "    #Transpose the dataframe to fulfilled the requirenment \n",
    "    rev_growth_df = growth_df.set_index('Company').T\n",
    "    \n",
    "    #return sales_trans_df,cost_trans_df,gross_margin_trans_df,rev_growth_df,q4_sales_trans_df,q4_cost_trans_df\n",
    "    return q4_sales_trans_df,q4_cost_trans_df, gross_margin_trans_df,rev_growth_df\n",
    "#%%%"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "37432872",
   "metadata": {},
   "outputs": [],
   "source": [
    "# for q4\n",
    "def q4_calculation(net_sales_10q,cost_of_service_10q,sales_df,service_df,company_values):\n",
    "    q4_sales_lst = []\n",
    "    q4_cost_lst = []\n",
    "    for company in company_values:\n",
    "        comp = company.replace(\" \", \"\")\n",
    "#         print(comp+\"Sales\")\n",
    "        #First we calculate for net sales\n",
    "        q3 = int(locale.atoi(net_sales_10q.loc[net_sales_10q.index[2], comp]))\n",
    "#         print(str(q3) + \" Sales\"+comp)\n",
    "        q2 = int(locale.atoi(net_sales_10q.loc[net_sales_10q.index[3], comp]))\n",
    "        q1 = int(locale.atoi(net_sales_10q.loc[net_sales_10q.index[4], comp]))\n",
    "        print('sales_df')\n",
    "        print(sales_df)\n",
    "        annual_value = int(sales_df.loc[sales_df['Company'] == company, 'Net sales values(M$)'].iloc[0])\n",
    "        q4_value = annual_value - (q1+q2+q3)\n",
    "        q4_sales_lst.append(q4_value)\n",
    "        #Second we calculate for cost of service\n",
    "        comp1 = company.replace(\" \", \"\")\n",
    "#         print(str(type(cost_of_service_10q.loc[cost_of_service_10q.index[2], comp1]))+\"TYPE\")\n",
    "        if type(cost_of_service_10q.loc[cost_of_service_10q.index[2], comp1])==np.float64:\n",
    "            q3_service = round(cost_of_service_10q.loc[cost_of_service_10q.index[2], comp1])\n",
    "#             print(str(q3_service) + \" Service\")\n",
    "            q2_service = round(cost_of_service_10q.loc[cost_of_service_10q.index[3], comp1])\n",
    "            q1_service = round(cost_of_service_10q.loc[cost_of_service_10q.index[4], comp1])\n",
    "            annual_value_service = int(service_df.loc[service_df['Company'] == company, 'Cost of service values(M$)'].iloc[0])\n",
    "            q4_value_service = annual_value_service - (q1_service+q2_service+q3_service)\n",
    "            q4_cost_lst.append(q4_value_service)\n",
    "        else:\n",
    "            print('comp1')\n",
    "            print(comp1)\n",
    "            print(cost_of_service_10q.loc[cost_of_service_10q.index[2], comp1])\n",
    "            q3_service = int(locale.atoi(cost_of_service_10q.loc[cost_of_service_10q.index[2], comp1]))\n",
    "#             print(str(q3_service) + \" Service\"+comp1)\n",
    "            q2_service = int(locale.atoi(cost_of_service_10q.loc[cost_of_service_10q.index[3], comp1]))\n",
    "            q1_service = int(locale.atoi(cost_of_service_10q.loc[cost_of_service_10q.index[4], comp1]))\n",
    "            annual_value_service = int(service_df.loc[service_df['Company'] == company, 'Cost of service values(M$)'].iloc[0])\n",
    "            q4_value_service = annual_value_service - (q1_service+q2_service+q3_service)\n",
    "            q4_cost_lst.append(q4_value_service)\n",
    "                                         \n",
    "    return q4_sales_lst,q4_cost_lst"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "id": "80c0185c",
   "metadata": {},
   "outputs": [],
   "source": [
    "#use for q4\n",
    "def calc_revenue_growth_q4(net_sales,sales_df,company_list):\n",
    "    '''This function takes the current net sales values(i.e.-from sales_df) and\n",
    "    previous net sales values(i.e.-from net_sales_10q) and company list as a \n",
    "    input and return the revenue growth of all companies as a list'''\n",
    "    revenue_growth_values = []\n",
    "    for company in company_list:\n",
    "        revenue_growth = round((int(sales_df.loc[sales_df['Company'] == company, 'Net sales(M$)'].iloc[0]) - int(locale.atoi(net_sales.loc[net_sales.index[0], company])))/int(locale.atoi(net_sales.loc[net_sales.index[0], company]))*100,2)\n",
    "        revenue_growth_values.append(revenue_growth)\n",
    "    return revenue_growth_values\n",
    "#%%%"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "id": "236fc20f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def update_10_q():\n",
    "    #file_location = \"C:\\\\Users\\\\deepali.b\\\\DL_tensorflow\\\\10k_document\"\n",
    "    file_location = '10q_extraction_new'\n",
    "    #crete_temp_folder(file_location)\n",
    "\n",
    "    company_to_update = get_time_update()\n",
    "    company_to_update = ['MDT','STE','SYK','JNJ','GMED']\n",
    "    if len(company_to_update)>0:\n",
    "        # extract from code and update in pickle also update time pickl    \n",
    "\n",
    "        with open(DIR_PATH+'net_sales_10q.pkl', 'rb') as f:\n",
    "            net_sales_10q = pickle.load(f)\n",
    "        with open(DIR_PATH+'sequential_quarter_growth.pkl', 'rb') as f:\n",
    "            sequential_quarter_growth_10q = pickle.load(f)\n",
    "        with open(DIR_PATH+'gross_margin_10_q.pkl', 'rb') as f:\n",
    "            gross_margin_10q = pickle.load(f)\n",
    "        with open(DIR_PATH+'cost_of_service_10q.pkl', 'rb') as f:\n",
    "            cost_of_service_10q = pickle.load(f)\n",
    "\n",
    "        last_q = net_sales_10q['Quarters/ USD millions'][0]\n",
    "        print('last_q')\n",
    "        print(last_q)\n",
    "        print()\n",
    "\n",
    "        if any(net_sales_10q.iloc[0].isna()):\n",
    "            new_quarter_year = last_q\n",
    "        else:\n",
    "            new_quarter_year = get_quarter(last_q)\n",
    "\n",
    "        column_name_m = [elem+'(M$)' for elem in company_to_update]\n",
    "        column_name_p = [elem+'(%)' for elem in company_to_update]\n",
    "\n",
    "        net_sales_dict = {net_sales_10q.columns[0]: [new_quarter_year]}\n",
    "        net_sales_dict.update({elem:np.nan for elem in column_name_m})\n",
    "        cost_of_service_dict = {cost_of_service_10q.columns[0]: [new_quarter_year]}\n",
    "        cost_of_service_dict.update({elem:np.nan for elem in column_name_m})\n",
    "\n",
    "        revenue_grth_dict = {sequential_quarter_growth_10q.columns[0]: [new_quarter_year]}\n",
    "        revenue_grth_dict.update({elem:np.nan for elem in column_name_p})\n",
    "        gross_margin_dict = {gross_margin_10q.columns[0]: [new_quarter_year]}\n",
    "        gross_margin_dict.update({elem:np.nan for elem in column_name_p})\n",
    "\n",
    "        sales_trans_df = pd.DataFrame()\n",
    "        cost_trans_df = pd.DataFrame()\n",
    "        gross_margin_trans_df = pd.DataFrame()\n",
    "        rev_growth_df = pd.DataFrame()\n",
    "\n",
    "        print('company_to_update')\n",
    "        print(company_to_update)\n",
    "        for company in company_to_update:\n",
    "            company_list = []\n",
    "            #and any(net_sales_10qn['Quarters/ USD millions'].isin([new_quarter_year])):\n",
    "            if any(pd.isnull(elem) for elem in list(net_sales_dict.values())):\n",
    "                company_elem = re.sub(r\" ?\\([^)]+\\)\", \"\", company)\n",
    "                company_list.append(company_elem)\n",
    "                print('company_list')\n",
    "                print(company_list)\n",
    "                print(download_files(DIR_PATH+file_location, company_elem, '10-Q'))\n",
    "                print(download_files(DIR_PATH+file_location, company_elem, '10-K'))\n",
    "                latest_date_report = extract_file_date(file_location,company_elem)\n",
    "                print('latest_date_report')\n",
    "                print(latest_date_report)\n",
    "                date_now = datetime.datetime.now()\n",
    "                if '10-Q' in latest_date_report.keys():\n",
    "                    date_10_q = datetime.datetime.strptime(latest_date_report['10-Q'], '%Y-%m-%d')\n",
    "                    date_diff = (date_now-date_10_q).days\n",
    "                    print('date_diff 10-q')\n",
    "                    print(date_diff)\n",
    "                    if date_diff >10: # >10\n",
    "                        if '10-K' in latest_date_report.keys():\n",
    "                            print('checking 10-k')\n",
    "                            date_10_k = datetime.datetime.strptime(latest_date_report['10-K'], '%Y-%m-%d')\n",
    "                            date_diff = (date_now-date_10_k).days\n",
    "                            print('Date difference for 10-k')\n",
    "                            print(date_diff)\n",
    "                            print(\"Download 10-k and extract\")\n",
    "                            if 1< date_diff <10:\n",
    "                                print(\"Extracting from 10-k\")\n",
    "                                form_type = '10-K'\n",
    "                                sales_trans_df,cost_trans_df,gross_margin_trans_df,rev_growth_df = calling_function_10k(file_location,company_list,form_type)\n",
    "                                #k_data = calling_function_10k(file_location,company_list,form_type)\n",
    "                                #print(k_data)\n",
    "                                print(\"Download and extract from 10-k here update for Q4\")\n",
    "                                print(sales_trans_df)\n",
    "                                print(cost_trans_df)\n",
    "                                print(gross_margin_trans_df)\n",
    "                                print(rev_growth_df)\n",
    "                            else:\n",
    "                                print(\"No latest report available\")\n",
    "                    else:\n",
    "                        form_type = '10-Q'\n",
    "                        print(\"Download and extract from 10-q here\")\n",
    "                        print('company_list')\n",
    "                        print(company_list)\n",
    "\n",
    "\n",
    "                        sales_trans_df,cost_trans_df,gross_margin_trans_df,rev_growth_df = calling_function(file_location,company_list,form_type)\n",
    "\n",
    "                if sales_trans_df.empty or cost_trans_df.empty or gross_margin_trans_df.empty or rev_growth_df.empty:\n",
    "                    print(\"No update in data\")\n",
    "                else:\n",
    "                    column_name_m = company+'(M$)'\n",
    "                    column_name_p = company+'(%)'\n",
    "                    if any(net_sales_10q['Quarters/ USD millions'].isin([new_quarter_year])):\n",
    "                        print(\"Updating existing row\")\n",
    "                        net_sales_10q.loc[0,column_name_m] = sales_trans_df[column_name_m].iloc[0]\n",
    "                        sequential_quarter_growth_10q.loc[0,column_name_p] = rev_growth_df[column_name_p].iloc[0]\n",
    "                        gross_margin_10q.loc[0,column_name_p] = gross_margin_trans_df[column_name_p].iloc[0]\n",
    "                        cost_of_service_10q.loc[0,column_name_m] = cost_trans_df[column_name_m].iloc[0]\n",
    "    #                     net_sales_10qn = net_sales_10q\n",
    "    #                     sequential_quarter_growth_10qn = sequential_quarter_growth_10q\n",
    "    #                     gross_margin_10qn = gross_margin_10q\n",
    "    #                     cost_of_service_10qn = cost_of_service_10q\n",
    "                    else:\n",
    "                        print(\"creating new row\")\n",
    "                        net_sales_update = pd.DataFrame({net_sales_10q.columns[0]: [new_quarter_year], column_name_m: sales_trans_df[column_name_m].iloc[0]})\n",
    "                        sequential_update = pd.DataFrame({sequential_quarter_growth_10q.columns[0]: [new_quarter_year], column_name_p: rev_growth_df[column_name_p].iloc[0]})\n",
    "                        gross_margin_update = pd.DataFrame({gross_margin_10q.columns[0]: [new_quarter_year], column_name_p: gross_margin_trans_df[column_name_p].iloc[0]})\n",
    "                        cost_of_ser_update = pd.DataFrame({cost_of_service_10q.columns[0]: [new_quarter_year], column_name_m: cost_trans_df[column_name_m].iloc[0]})\n",
    "\n",
    "                        net_sales_10q = pd.concat([net_sales_update,net_sales_10q],ignore_index=True)\n",
    "                        sequential_quarter_growth_10q = pd.concat([sequential_update,sequential_quarter_growth_10q],ignore_index=True)\n",
    "                        gross_margin_10q = pd.concat([gross_margin_update,gross_margin_10q],ignore_index=True)\n",
    "                        cost_of_service_10q = pd.concat([cost_of_ser_update,cost_of_service_10q], ignore_index=True)\n",
    "\n",
    "    #                     net_sales_10q.loc[0,elem] = sales_trans_df[column_name_m].iloc[0].values[0]\n",
    "    #                     sequential_quarter_growth_10q.loc[0,sequential_quarter_growth_10q.columns[0]] = rev_growth_df[column_name_p].iloc[0].values[0]\n",
    "    #                     gross_margin_10q.loc[0,gross_margin_10q.columns[0]] = gross_margin_trans_df[column_name_p].iloc[0].values[0]\n",
    "    #                     cost_of_service_10q.loc[0,cost_of_service_10q.columns[0]] = cost_trans_df[column_name_m].iloc[0].values[0]\n",
    "\n",
    "                    with open('net_sales_10q.pkl', 'wb') as f:\n",
    "                        pickle.dump(net_sales_10qn, f)\n",
    "                    with open('sequential_quarter_growth.pkl', 'wb') as f:\n",
    "                        pickle.dump(sequential_quarter_growth_10qn, f)\n",
    "                    with open('gross_margin_10_q.pkl', 'wb') as f:\n",
    "                        pickle.dump(gross_margin_10qn, f)\n",
    "                    with open('cost_of_service_10q.pkl', 'wb') as f:\n",
    "                        pickle.dump(cost_of_service_10qn, f)\n",
    "\n",
    "                    print(sales_trans_df)\n",
    "                    print('\\n')\n",
    "                    print(cost_trans_df)\n",
    "                    print('\\n')\n",
    "                    print(gross_margin_trans_df)\n",
    "                    print('\\n')\n",
    "                    print(rev_growth_df)\n",
    "\n",
    "\n",
    "    print('new_quarter')\n",
    "    print(new_quarter_year)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "id": "fe418909",
   "metadata": {},
   "outputs": [],
   "source": [
    "#######"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3e1d53c1",
   "metadata": {},
   "source": [
    "##### 10k download and update"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "81660b00",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 149,
   "id": "ebdce252",
   "metadata": {},
   "outputs": [],
   "source": [
    "def report_10k(path,company_ticker,File_Type,year):\n",
    "    try:\n",
    "        year = str(year)\n",
    "        input_date = year+'-01' + '-01'\n",
    "        date_format = datetime.datetime.strptime(input_date, \"%Y-%m-%d\")\n",
    "        start_date = str(date_format).split(\" \")[0]\n",
    "        date_next = date_format + relativedelta(months=12)\n",
    "        end_date = str(date_next).split(\" \")[0] \n",
    "        dl_Period = Downloader(path)\n",
    "        print('company_ticker')\n",
    "        print(company_ticker)\n",
    "        dl_Period.get(File_Type,company_ticker,after= start_date, before=end_date)\n",
    "        print(\"File downloaded successfully at given path\")\n",
    "    except Exception as e:\n",
    "        print(e)\n",
    "        print(\"Error in downloading file\")\n",
    "        \n",
    "def download_10k_update(path, company_ticker, File_Type, year): \n",
    "#     global path\n",
    "#     path = input('Please input the path:')\n",
    "#     global company_ticker\n",
    "#     company_ticker = input('Please input the Company Ticker:')\n",
    "#     File_Type = input('Please input the File Type:')\n",
    "#     year = input('Please input the Year:')\n",
    "    report_10k(path,company_ticker,File_Type,year)\n",
    "    root = path\n",
    "    pattern = \"*.txt\"\n",
    "    #global path_list\n",
    "    \n",
    "    for path, subdirs, files in os.walk(root):\n",
    "        for name in files:\n",
    "            if fnmatch(name, pattern):\n",
    "                path_list = []\n",
    "                print('os.path.join(path, name')\n",
    "                print(os.path.join(path, name))\n",
    "                path_list.append(os.path.join(path, name))\n",
    "                path_list = str(''.join(path_list))\n",
    "    path_list\n",
    "    return path_list \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 150,
   "id": "59d06ec9",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_content_10k(path_list):\n",
    "    with open(path_list) as f:\n",
    "        global raw_10k\n",
    "        raw_10k = f.read()\n",
    "        raw_10k\n",
    "\n",
    "    # Regex to find <DOCUMENT> tags\n",
    "    doc_start_pattern = re.compile(r'<DOCUMENT>')\n",
    "    doc_end_pattern = re.compile(r'</DOCUMENT>')\n",
    "    # Regex to find <TYPE> tag prceeding any characters, terminating at new line\n",
    "    type_pattern = re.compile(r'<TYPE>[^\\n]+')\n",
    "\n",
    "    # Create 3 lists with the span idices for each regex\n",
    "    doc_start_is = [x.end() for x in doc_start_pattern.finditer(raw_10k)]\n",
    "    doc_end_is = [x.start() for x in doc_end_pattern.finditer(raw_10k)]\n",
    "    doc_types = [x[len('<TYPE>'):] for x in type_pattern.findall(raw_10k)]\n",
    "\n",
    "    global document\n",
    "    document = {}\n",
    "    for doc_type, doc_start, doc_end in zip(doc_types, doc_start_is, doc_end_is):\n",
    "        if doc_type == '10-K':\n",
    "            document[doc_type] = raw_10k[doc_start:doc_end]\n",
    "    #print(document['10-K'][:500])\n",
    "\n",
    "    #Regex pattern for finance table\n",
    "    regex = re.compile(r'(Consolidated Statements of Income)|(CONSOLIDATED STATEMENTS OF INCOME)|(CONSOLIDATED STATEMENTS OF EARNINGS)|(CONSOLIDATED STATEMENTS OF OPERATIONS AND COMPREHENSIVE INCOME)|(CONSOLIDATED STATEMENTS OF OPERATIONS)')\n",
    "\n",
    "    # Use finditer to math the regex\n",
    "    matches = regex.finditer(document['10-K'])\n",
    "\n",
    "    # Write a for loop to print the matches\n",
    "    for match in matches:\n",
    "        match\n",
    "        \n",
    "    # Matches\n",
    "    matches = regex.finditer(document['10-K'])\n",
    "\n",
    "    # Create the dataframe\n",
    "    test_df = pd.DataFrame([(x.group(), x.start(), x.end()) for x in matches])\n",
    "    test_df.columns = ['item', 'start', 'end']\n",
    "\n",
    "    #Replacing the end value to none\n",
    "    test_df['end'] = test_df['end'].replace('','',regex=True,inplace=True)\n",
    "\n",
    "    #Replacing the financial metric name to parameter\n",
    "    test_df.loc[0,\"item\"] = \"Parameter\"\n",
    "    #display(test_df.head())\n",
    "\n",
    "    # Drop duplicates\n",
    "    pos_dat = test_df.sort_values('start', ascending=True).drop_duplicates(subset=['item'])\n",
    "\n",
    "    # Set item as the dataframe index\n",
    "    pos_dat.set_index('item', inplace=True)\n",
    "    \n",
    "    #Get item 8\n",
    "    item_8_raw = document['10-K'][pos_dat['start'].loc['Parameter']:pos_dat['end'].loc['Parameter']]\n",
    "    #display(item_8_raw[0:1000])\n",
    "\n",
    "    # First convert the raw text we have to exrtacted to BeautifulSoup object \n",
    "    Fin_8A = BeautifulSoup(item_8_raw, 'lxml')\n",
    "    #display(Fin_8A)\n",
    "\n",
    "    # Finding out metrices using tags\n",
    "    Fin_headings=[]\n",
    "    for i in Fin_8A.findAll('table'):\n",
    "        Fin_headings.append(i.text)\n",
    "    for i in Fin_8A.findAll('span'):\n",
    "        Fin_headings.append(i.text)\n",
    "    for i in Fin_8A.findAll('td'):\n",
    "        Fin_headings.append(i.text)\n",
    "    Fin_headings = ''.join(Fin_headings)\n",
    "    global Fin_text\n",
    "    Fin_text = unicodedata.normalize(\"NFKD\",Fin_headings)\n",
    "\n",
    "def extract_10k(path_list):\n",
    "    get_content_10k(path_list)\n",
    "    print('path_list')\n",
    "    print(path_list)\n",
    "    path=os.path.dirname(path_list)\n",
    "    global Ticker\n",
    "    Ticker = os.path.basename(os.path.dirname(os.path.dirname(path)))\n",
    "    Ticker\n",
    "    print(\"Data Extraction is successfull!\")\n",
    "\n",
    "def Net_Sales():\n",
    "    results =[]\n",
    "    try:\n",
    "        Reg = re.findall(r'(?P<name>(Total.revenues)[\\W\\d]*)',Fin_text) \n",
    "        text = (Reg[-1][0])\n",
    "        results.append(text)\n",
    "        return results\n",
    "    except:\n",
    "        pass\n",
    "    \n",
    "    try:\n",
    "        Reg = re.findall(r'(?P<name>(Net.sales|Sales.to.customers|Net.Sales|Sales)[\\W\\d]*)',Fin_text) \n",
    "        text = (Reg[0][0])\n",
    "        results.append(text)\n",
    "        return results \n",
    "        \n",
    "    except:\n",
    "        pass\n",
    "\n",
    "\n",
    "def sales():\n",
    "    NS = str(Net_Sales())\n",
    "    Net = re.sub(r',', '', NS)\n",
    "    Net = re.findall('[\\d]*[\\d]+',Net)\n",
    "    global NetSales\n",
    "    NetSales = pd.DataFrame(Net).transpose()\n",
    "    Date = re.findall(r'<ACCEPTANCE-DATETIME>\\d{4}',raw_10k)\n",
    "    Year = re.findall(r'\\d',str(Date))\n",
    "    s=[str(i) for i in Year]\n",
    "    global years\n",
    "    years = int(\"\".join(s))\n",
    "    NetSales.columns=[years,years-1,years-2]\n",
    "    NetSales.insert(0, \"YEAR\", Ticker +' (M$)')\n",
    "    if Ticker == 'STE':\n",
    "        NetSales.iloc[0,1] =round(float(NetSales.iloc[0,1])*0.001)\n",
    "        NetSales.iloc[0,1] =(str(NetSales.iloc[0,1]))\n",
    "        NetSales.iloc[0,2] =round(float(NetSales.iloc[0,2])*0.001)\n",
    "        NetSales.iloc[0,2] =(str(NetSales.iloc[0,2]))\n",
    "        NetSales.iloc[0,3] =round(float(NetSales.iloc[0,3])*0.001)\n",
    "        NetSales.iloc[0,3] =(str(NetSales.iloc[0,3]))\n",
    "    elif Ticker == 'GMED':\n",
    "        NetSales.iloc[0,1] =round(float(NetSales.iloc[0,1])*0.001)\n",
    "        NetSales.iloc[0,1] =(str(NetSales.iloc[0,1]))\n",
    "        NetSales.iloc[0,2] =round(float(NetSales.iloc[0,2])*0.001)\n",
    "        NetSales.iloc[0,2] =(str(NetSales.iloc[0,2]))\n",
    "        NetSales.iloc[0,3] =round(float(NetSales.iloc[0,3])*0.001)\n",
    "        NetSales.iloc[0,3] =(str(NetSales.iloc[0,3]))\n",
    "    else:\n",
    "        NetSales.iloc[0,1] =(str(NetSales.iloc[0,1]))\n",
    "        NetSales.iloc[0,2] =(str(NetSales.iloc[0,2]))\n",
    "        NetSales.iloc[0,3] =(str(NetSales.iloc[0,3]))\n",
    "    NetSales = NetSales.transpose()\n",
    "    NetSales.columns = NetSales.iloc[0]\n",
    "    NetSales = NetSales[1:]\n",
    "    return NetSales[0:1]\n",
    "\n",
    "def Cost_of_Revenue():\n",
    "    Results = []\n",
    "    try:\n",
    "        Reg = re.findall(r'(?P<name>(Cost.of.products.sold,.excluding.amortization.of.intangible.assets|Cost.of.products.sold|Cost.of.goods.sold|Cost.of.sales|Total.cost.of.revenues)[\\W\\d]*)',Fin_text) #MDT SYK \n",
    "        text = (Reg[0][0])\n",
    "        Results.append(text)\n",
    "        return Results\n",
    "    except:\n",
    "        pass\n",
    "\n",
    "def Revenuecost():\n",
    "    Rev = str(Cost_of_Revenue())\n",
    "    RevC = re.sub(r',', '', Rev)\n",
    "    REG = re.findall('[\\d]*[\\d]+',RevC)\n",
    "    global Revcos\n",
    "    Revcos = pd.DataFrame(REG).transpose()\n",
    "    Date = re.findall(r'<ACCEPTANCE-DATETIME>\\d{4}',raw_10k)\n",
    "    Year = re.findall(r'\\d',str(Date))\n",
    "    s=[str(i) for i in Year]\n",
    "    years = int(\"\".join(s))\n",
    "    Revcos.columns=[years,years-1,years-2]\n",
    "    Revcos.insert(0, \"YEAR\", Ticker +' (M$)')\n",
    "    if Ticker == 'STE':\n",
    "        Revcos.iloc[0,1] =round(float(Revcos.iloc[0,1])*0.001)\n",
    "        Revcos.iloc[0,1] =(str(Revcos.iloc[0,1]))\n",
    "        Revcos.iloc[0,2] =round(float(Revcos.iloc[0,2])*0.001)\n",
    "        Revcos.iloc[0,2] =(str(Revcos.iloc[0,2]))\n",
    "    elif Ticker == 'GMED':\n",
    "        Revcos.iloc[0,1] =round(float(Revcos.iloc[0,1])*0.001)\n",
    "        Revcos.iloc[0,1] =(str(Revcos.iloc[0,1]))\n",
    "        Revcos.iloc[0,2] =round(float(Revcos.iloc[0,2])*0.001)\n",
    "        Revcos.iloc[0,2] =(str(Revcos.iloc[0,2]))\n",
    "    else:\n",
    "        Revcos.iloc[0,1] =(str(Revcos.iloc[0,1]))\n",
    "        Revcos.iloc[0,2] =(str(Revcos.iloc[0,2]))\n",
    "    Revcos = Revcos.transpose()\n",
    "    Revcos.columns = Revcos.iloc[0]\n",
    "    Revcos = Revcos[1:]\n",
    "    return Revcos[0:1]\n",
    "\n",
    "\n",
    "def Gross_Margin():\n",
    "    sales()\n",
    "    global Sales\n",
    "    Sales = NetSales.copy()\n",
    "    Sales.iloc[0,0] = Sales.iloc[0,0].replace(',', '')\n",
    "    Sales.iloc[1,0] = Sales.iloc[1,0].replace(',', '')\n",
    "    Sales.iloc[2,0] = Sales.iloc[2,0].replace(',', '')\n",
    "    Date = re.findall(r'<ACCEPTANCE-DATETIME>\\d{4}',raw_10k)\n",
    "    Year = re.findall(r'\\d',str(Date))\n",
    "    s=[str(i) for i in Year]\n",
    "    years = int(\"\".join(s))\n",
    "    RevC = Revcos.copy()\n",
    "    RevC.iloc[0,0] = RevC.iloc[0,0].replace(',', '')\n",
    "    RevC.iloc[1,0] = RevC.iloc[1,0].replace(',', '')\n",
    "    RevC.iloc[2,0] = RevC.iloc[2,0].replace(',', '')\n",
    "    Y1 = round(((float(Sales.iloc[0,0])-(float(RevC.iloc[0,0])))/(float(Sales.iloc[0,0]))*100),2)\n",
    "    Y2 = round(((float(Sales.iloc[1,0])-(float(RevC.iloc[1,0])))/(float(Sales.iloc[1,0]))*100),2)\n",
    "    Y3 = round(((float(Sales.iloc[2,0])-(float(RevC.iloc[2,0])))/(float(Sales.iloc[2,0]))*100),2)\n",
    "    grossmargin = pd.DataFrame([[years,Y1],[years-1,Y2],[years-2,Y3]])\n",
    "    grossmargin.columns=[\"YEAR\", Ticker + ' (%)']\n",
    "    return grossmargin[0:1]\n",
    "\n",
    "def Revenue_growth():\n",
    "    RevSales = NetSales.copy()\n",
    "    RevSales.iloc[0,0] = RevSales.iloc[0,0].replace(',', '')\n",
    "    RevSales.iloc[1,0] = RevSales.iloc[1,0].replace(',', '')\n",
    "    RevSales.iloc[2,0] = RevSales.iloc[2,0].replace(',', '')\n",
    "    B =round(((((float(RevSales.iloc[0,0]))/(float(RevSales.iloc[1,0])))-1)*100),2)\n",
    "    C =round(((((float(RevSales.iloc[1,0]))/(float(RevSales.iloc[2,0])))-1)*100),2)\n",
    "    RG = pd.DataFrame([[years,B],[years-1,C]])\n",
    "    RG.columns = [\"YEAR\",Ticker + \" (%)\"]\n",
    "    return RG[0:1]\n",
    "   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 166,
   "id": "7dfdd9e7",
   "metadata": {},
   "outputs": [],
   "source": [
    "def crete_temp_folder(folder_path):\n",
    "    #folder_path = '10k_download/'\n",
    "    if os.path.exists(folder_path):\n",
    "        shutil.rmtree(folder_path)\n",
    "        #os.makedirs(newpath)\n",
    "    else:\n",
    "        os.makedirs(folder_path)\n",
    "    return folder_path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 167,
   "id": "688c7168",
   "metadata": {},
   "outputs": [],
   "source": [
    "def update_10_k(year,comapny_name,form_type):\n",
    "    path_10k = DIR_PATH+\"10k_downlod_extraction/\"\n",
    "    with open(DIR_PATH+'revenue_growth.pkl', 'rb') as f:\n",
    "        revenue_growth_df = pickle.load(f)\n",
    "    with open(DIR_PATH+'gross_margin.pkl', 'rb') as f:\n",
    "        gross_margin_df = pickle.load(f)\n",
    "    with open(DIR_PATH+'net_sales.pkl', 'rb') as f:\n",
    "        net_sales_df = pickle.load(f)\n",
    "\n",
    "    \n",
    "    print('company_name')\n",
    "    print(comapny_name)\n",
    "    path_list = download_10k_update(path_10k, comapny_name, form_type, year)\n",
    "    extract_10k(path_list)\n",
    "    sales_df = sales()\n",
    "    revenue_cost_df = Revenuecost()\n",
    "    gross_df = Gross_Margin()\n",
    "    print('gross_df')\n",
    "    print(gross_df)\n",
    "    print(gross_margin_df.iloc[0].values[1])\n",
    "    revenue_df = Revenue_growth()\n",
    "    print('revenue_df')\n",
    "    print(revenue_df)\n",
    "    print(revenue_df.iloc[0].values[1])\n",
    "    company_name_m = comapny_name + ' (M$)'\n",
    "    print('company_name_m')\n",
    "    print(company_name_m)\n",
    "    company_name_p = comapny_name + ' (%)'\n",
    "    print(company_name_p)\n",
    "    if revenue_growth_df['Year'].isin([year]).any():\n",
    "        revenue_growth_df.set_index(\"Year\",inplace=True)\n",
    "        gross_margin_df.set_index(\"Year\",inplace=True)\n",
    "        net_sales_df.set_index(net_sales_df.columns[0],inplace=True)\n",
    "        print(net_sales_df.columns)\n",
    "        print('updating existing row')\n",
    "        print(revenue_df.iloc[0].values[1])\n",
    "        print(revenue_df)\n",
    "        print(sales_df.index)\n",
    "        print(sales_df.iloc[0].values[0])\n",
    "\n",
    "        revenue_growth_df.loc[year,company_name_p] = revenue_df.iloc[0].values[1]\n",
    "        gross_margin_df.loc[year,company_name_p] = gross_df.iloc[0].values[1]\n",
    "        net_sales_df.loc[year,company_name_m] = int(sales_df.iloc[0].values[0])\n",
    "        #net_sales_df.loc[year,company_name_m] = 0\n",
    "        print(net_sales_df)\n",
    "        revenue_growth_df.reset_index(inplace= True)\n",
    "        gross_margin_df.reset_index(inplace= True)\n",
    "        net_sales_df.reset_index(inplace= True)\n",
    "\n",
    "        #gross_margin_df[company_name_p].iloc[0] = gross_df.iloc[0].values[1]\n",
    "        #net_sales_df[company_name_m].iloc[0] = int(sales_df.iloc[0].values[0])\n",
    "    else:\n",
    "        print('adding new row')\n",
    "        print(revenue_df.iloc[0].values[0])\n",
    "        revenue_growth_update = pd.DataFrame({revenue_growth_df.columns[0]: [year], company_name_p: revenue_df.iloc[0].values[1]})\n",
    "        gross_margin_update = pd.DataFrame({gross_margin_df.columns[0]: [year], company_name_p: gross_df.iloc[0].values[1]})\n",
    "        net_sales_update = pd.DataFrame({net_sales_df.columns[0]: [year], company_name_m: int(sales_df.iloc[0].values[0])})\n",
    "        print('net_sales_update')\n",
    "        print(net_sales_update)\n",
    "        revenue_growth_df = pd.concat([revenue_growth_update,revenue_growth_df],ignore_index=True)\n",
    "        gross_margin_df = pd.concat([gross_margin_update,gross_margin_df],ignore_index=True)\n",
    "        print(\"-----------------\")\n",
    "        print(net_sales_update.index)\n",
    "        print(net_sales_df.index)\n",
    "        net_sales_df = pd.concat([net_sales_update,net_sales_df],ignore_index=True)\n",
    "        print('net_sales_df new row')\n",
    "        print(net_sales_df)\n",
    "    print('updating pickle file')\n",
    "    with open(DIR_PATH+'revenue_growth.pkl', 'wb') as f:\n",
    "        pickle.dump(revenue_growth_df, f)\n",
    "    with open(DIR_PATH+'gross_margin.pkl', 'wb') as f:\n",
    "        pickle.dump(gross_margin_df, f)\n",
    "    with open(DIR_PATH+'net_sales.pkl', 'wb') as f:\n",
    "        pickle.dump(net_sales_df, f)\n",
    "    crete_temp_folder(path_10k)\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8e378d6f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 153,
   "id": "53375433",
   "metadata": {},
   "outputs": [],
   "source": [
    "######"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 154,
   "id": "6255c6e2",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get10kurl(year, form_type, company_ticker_name):\n",
    "    \"\"\" Function to get html link of 10-K for given company and year\n",
    "    \"\"\"\n",
    "    queryApi = QueryApi(api_key= API_KEY)\n",
    "    year = str(year)\n",
    "    print('company_ticker_name',company_ticker_name)\n",
    "    try:\n",
    "        input_date = year+'-01' + '-01'\n",
    "        #print('input_date', input_date)\n",
    "        date_format = datetime.datetime.strptime(input_date, \"%Y-%m-%d\") + relativedelta(years=1)\n",
    "        date_format_1 = str(date_format).split(\" \")[0]\n",
    "        date_previous = date_format - relativedelta(years=2)\n",
    "        date_previous_1 = str(date_previous).split(\" \")[0]\n",
    "        \n",
    "        date_range = '{' + date_previous_1 + ' TO ' + date_format_1 + '}'\n",
    "        #print('date_range',date_range)\n",
    "    except Exception:\n",
    "        print(\"Check input year is correct or not\")\n",
    "    \n",
    "    try:\n",
    "        query = {\n",
    "          \"query\": { \"query_string\": { \n",
    "              \"query\": \"ticker\"+\":\" +company_ticker_name +\" AND formType:\" + \"\\\"\" + form_type + \"\\\"\" + \" AND filedAt:\" + date_range  \n",
    "            } },\n",
    "          \"from\": \"0\",\n",
    "          \"size\": \"10\",\n",
    "          \"sort\": [{ \"filedAt\": { \"order\": \"desc\" } }]\n",
    "        }\n",
    "\n",
    "        filings = queryApi.get_filings(query)\n",
    "    except Exception:\n",
    "        print(\"Check parameters in query\")\n",
    "        \n",
    "    return filings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 155,
   "id": "310be5d4",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_text_path(input_path):\n",
    "    root = input_path\n",
    "    pattern = \"*.txt\"\n",
    "    path_list = []\n",
    "    for path, subdirs, files in os.walk(root):\n",
    "        for name in files:\n",
    "            if fnmatch(name, pattern):\n",
    "                path_list.append(os.path.join(path, name))\n",
    "    #print('path_list',path_list)\n",
    "    return path_list        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 156,
   "id": "955b7bd0",
   "metadata": {},
   "outputs": [],
   "source": [
    "def download_10k_risk_factor(path,company_ticker,File_Type,year):\n",
    "    try:\n",
    "        year = str(year)\n",
    "        input_date = year+'-01' + '-01'\n",
    "        date_format = datetime.datetime.strptime(input_date, \"%Y-%m-%d\")\n",
    "        start_date_1 = date_format + relativedelta(years=1)\n",
    "        start_date = str(start_date_1).split(\" \")[0]\n",
    "        #print('Start date:', start_date)\n",
    "        date_next = date_format - relativedelta(years=1)\n",
    "        #print('date_next', date_next)\n",
    "        end_date = str(date_next).split(\" \")[0] \n",
    "        #print(\"End date:\", end_date)\n",
    "        dl_Period = Downloader(path)\n",
    "        dl_Period.get(File_Type,company_ticker,after= end_date, before=start_date)\n",
    "        time.sleep(10)\n",
    "        print(\"File downloaded successfully at given path\")\n",
    "    except Exception as e:\n",
    "        print(e)\n",
    "        print(\"Error in downloading file\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 157,
   "id": "579649a9",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_content(input_text_file_path):\n",
    "    with open(input_text_file_path) as f:\n",
    "        raw_10k = f.read()\n",
    "    # Regex to find <DOCUMENT> tags\n",
    "    doc_start_pattern = re.compile(r'<DOCUMENT>')\n",
    "    doc_end_pattern = re.compile(r'</DOCUMENT>')\n",
    "    # Regex to find <TYPE> tag prceeding any characters, terminating at new line\n",
    "    type_pattern = re.compile(r'<TYPE>[^\\n]+')\n",
    "    \n",
    "    # Create 3 lists with the span idices for each regex\n",
    "\n",
    "\n",
    "    doc_start_is = [x.end() for x in doc_start_pattern.finditer(raw_10k)]\n",
    "    doc_end_is = [x.start() for x in doc_end_pattern.finditer(raw_10k)]\n",
    "    doc_types = [x[len('<TYPE>'):] for x in type_pattern.findall(raw_10k)]\n",
    "    document = {}\n",
    "\n",
    "    for doc_type, doc_start, doc_end in zip(doc_types, doc_start_is, doc_end_is):\n",
    "        if doc_type == '10-K':\n",
    "            document[doc_type] = raw_10k[doc_start:doc_end]\n",
    "    return document"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 158,
   "id": "8a70ad41",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_risk_text(input_document):\n",
    "    regex = re.compile(r'(>Item(\\s|&#160;|&nbsp;)(1A|1B|7A|7|8)\\.{0,1})|(ITEM\\s(1A|1B|7A|7|8))')\n",
    "    matches = regex.finditer(input_document['10-K'])\n",
    "\n",
    "    # Create the dataframe\n",
    "    test_df = pd.DataFrame([(x.group(), x.start(), x.end()) for x in matches])\n",
    "\n",
    "    test_df.columns = ['item', 'start', 'end']\n",
    "    test_df['item'] = test_df.item.str.lower()\n",
    "    \n",
    "    test_df.replace('&#160;',' ',regex=True,inplace=True)\n",
    "    test_df.replace('&nbsp;',' ',regex=True,inplace=True)\n",
    "    test_df.replace(' ','',regex=True,inplace=True)\n",
    "    test_df.replace('\\.','',regex=True,inplace=True)\n",
    "    test_df.replace('>','',regex=True,inplace=True)\n",
    "    pos_dat = test_df.sort_values('start', ascending=True).drop_duplicates(subset=['item'], keep='last')\n",
    "    pos_dat.set_index('item', inplace=True)\n",
    "    item_1a_raw = input_document['10-K'][pos_dat['start'].loc['item1a']:pos_dat['start'].loc['item1b']]\n",
    "    riskhtml = bs.BeautifulSoup(item_1a_raw) \n",
    "    #riskhtml = item_1a_raw\n",
    "    risk_headings = []\n",
    "    for i in riskhtml.find_all('span', style=lambda x: x and 'font-weight:700;' in x):\n",
    "        risk_headings.append(i.text)\n",
    "    for i in riskhtml.find_all('span', style=lambda x: x and 'font-weight:bold;' in x):\n",
    "        risk_headings.append(i.text)\n",
    "    for i in riskhtml.find_all('font', style=lambda x: x and 'font-weight:bold;' in x):\n",
    "        risk_headings.append(i.text)\n",
    "    return risk_headings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 159,
   "id": "f0b3c52a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_risk_keywords(risk_factor):\n",
    "    risk_keyword = []\n",
    "    for i in range(len(risk_factor)):\n",
    "        text = risk_factor[i].lower()\n",
    "        # load a spaCy model, depending on language, scale, etc.\n",
    "        nlp = spacy.load(\"en_core_web_sm\")\n",
    "        # add PyTextRank to the spaCy pipeline\n",
    "        nlp.add_pipe(\"textrank\")\n",
    "        doc = nlp(text)\n",
    "        # examine the top-ranked phrases in the document\n",
    "        for phrase in doc._.phrases[:20]:\n",
    "            risk_keyword.append(phrase.text)\n",
    "    custom_stopwords = ['we','us','.','u.s.','irs']\n",
    "    stopword_list = [*STOPLIST, *custom_stopwords]\n",
    "    \n",
    "    risk_keyword_list = [elem.lower() for elem in risk_keyword if elem not in stopword_list]\n",
    "    risk_dict = Counter(risk_keyword_list)\n",
    "    sorted_keywords = risk_dict.most_common()\n",
    "    return sorted_keywords"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 160,
   "id": "c01f6854",
   "metadata": {},
   "outputs": [],
   "source": [
    "def arrange_path(list_of_ticker,download_folder):\n",
    "    path_list = get_text_path(download_folder)\n",
    "    path_arranged_list = []\n",
    "    for folder_path in path_list:\n",
    "        subdirname = os.path.basename(os.path.dirname(os.path.dirname(os.path.dirname(folder_path))))\n",
    "        subdirname1 = os.path.basename(os.path.dirname(folder_path))\n",
    "        subdirname1 = subdirname1.split('-')\n",
    "        base_year = '20'\n",
    "        file_year = base_year+subdirname1[1]\n",
    "        #print(file_year)\n",
    "        company_dict = {'company_name':subdirname, 'year':file_year, 'file_path':folder_path}\n",
    "        path_arranged_list.append(company_dict)\n",
    "        #print(path_arranged_list)\n",
    "    return path_arranged_list\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 161,
   "id": "6f6de882",
   "metadata": {},
   "outputs": [],
   "source": [
    "def risk_factor_compare(list_of_ticker,year):\n",
    "    folder_path = '10k_download/'\n",
    "    downloaded_path = crete_temp_folder(folder_path)\n",
    "    File_type = '10-K'\n",
    "    for company_ticker in list_of_ticker:\n",
    "        try:\n",
    "            download_10k_risk_factor(downloaded_path,company_ticker,File_type,year)\n",
    "        except Exception as e:\n",
    "            print(e)\n",
    "    \n",
    "    arranged_path_dict = arrange_path(list_of_ticker, downloaded_path)\n",
    "    \n",
    "    #text_file_path = get_text_path(downloaded_path)\n",
    "    print('text_file_path')\n",
    "    #print(text_file_path)\n",
    "    path_list = []\n",
    "    risk_list = []\n",
    "    risk_keywords = []\n",
    "    \n",
    "    for elem in arranged_path_dict:\n",
    "        \n",
    "        html_content = get_content(elem['file_path'])\n",
    "        risk_factors_text = get_risk_text(html_content)\n",
    "        elem['risk_text'] = [risk_factors_text]\n",
    "        elem['risk_keywords'] = [get_risk_keywords(risk_factors_text)]\n",
    "        \n",
    "    #risk_data = pd.DataFrame(arranged_path_dict)\n",
    "    \n",
    "    #risk_data_text = risk_data.copy()\n",
    "    #risk_data_text1 = risk_data_text.drop(columns=['risk_keywords'])\n",
    "    #risk_data1 = risk_data.drop(columns=['risk_text'])\n",
    "    risk_data_text = pd.DataFrame(columns = [\"Year\",list_of_ticker[0]+\"_risk_text\",list_of_ticker[1]+\"_risk_text\"])\n",
    "    year_list = [elem[\"year\"] for elem in arranged_path_dict]\n",
    "    year_list1 = year_list[:int(len(year_list)/2)]\n",
    "    year_list2 = year_list[int(len(year_list)/2):]\n",
    "  \n",
    "    risk_data_text[\"Year\"] = year_list1\n",
    "    risk_data_text[list_of_ticker[0]+\"_risk_text\"] = [elem[\"risk_text\"] for elem in arranged_path_dict if elem['company_name']==list_of_ticker[0]]\n",
    "    risk_data_text[list_of_ticker[1]+\"_risk_text\"] = [elem[\"risk_text\"] for elem in arranged_path_dict if elem['company_name']==list_of_ticker[1]]\n",
    "    #print('lengths')\n",
    "    #print(len([elem[\"year\"] for elem in arranged_path_dict]))\n",
    "    #print(len([elem[\"risk_text\"] for elem in arranged_path_dict if elem['company_name']=='MDT']))\n",
    "    #print(len([elem[\"risk_text\"] for elem in arranged_path_dict if elem['company_name']=='STE']))\n",
    "    \n",
    "    risk_data_keyword = pd.DataFrame(columns = [\"Year\",list_of_ticker[0]+\"_risk_keywords\",list_of_ticker[1]+\"_risk_keywords\"])\n",
    "    risk_data_keyword[\"Year\"] = year_list2\n",
    "    risk_data_keyword[list_of_ticker[0]+\"_risk_keywords\"] = [elem[\"risk_keywords\"] for elem in arranged_path_dict if elem['company_name']==list_of_ticker[0]]\n",
    "    risk_data_keyword[list_of_ticker[1]+\"_risk_keywords\"] = [elem[\"risk_keywords\"] for elem in arranged_path_dict if elem['company_name']==list_of_ticker[1]]\n",
    "      \n",
    "      \n",
    "    print(\"Risk factors extracted successfully\")\n",
    "    print(\"len of risk_data\")\n",
    "    print(len(risk_data_text))\n",
    "    return (risk_data_text, risk_data_keyword)   "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9f1449d9",
   "metadata": {},
   "source": [
    "#### Download 10-K file to path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 162,
   "id": "09ae4f79",
   "metadata": {},
   "outputs": [],
   "source": [
    "def download_10k(path,company_ticker,File_Type,year, output_range):\n",
    "    try:\n",
    "        year = str(year)\n",
    "        print('path')\n",
    "        print(path)\n",
    "        input_date = year+'-01' + '-01'\n",
    "        date_format = datetime.datetime.strptime(input_date, \"%Y-%m-%d\")\n",
    "        start_date = str(date_format).split(\" \")[0]\n",
    "        print('Start date:', start_date)\n",
    "        #date_next = date_format + relativedelta(months=12)\n",
    "        date_previous = date_format - relativedelta(years=output_range)\n",
    "        #print('date_next', date_next)\n",
    "        end_date = str(date_previous).split(\" \")[0] \n",
    "        print(\"End date:\", end_date)\n",
    "        dl_Period = Downloader(path)\n",
    "        dl_Period.get(File_Type,company_ticker,after=end_date , before=start_date)         \n",
    "        print(\"File downloaded successfully at given path\")\n",
    "    except Exception as e:\n",
    "        print(e)\n",
    "        print(\"Error in downloading file\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "132d93b5",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 163,
   "id": "a80875b8",
   "metadata": {},
   "outputs": [],
   "source": [
    "def update_pickle_riskdf(risk_text_data, risk_keywords_data):\n",
    "    try:\n",
    "        with open('risk_text.pkl', 'rb') as f:\n",
    "            risk_text = pickle.load(f)\n",
    "        with open('risk_keywords.pkl', 'rb') as f:\n",
    "            risk_keywords = pickle.load(f)\n",
    "\n",
    "        risk_text_df_new = risk_text.append(risk_text_data)\n",
    "        risk_keywords_df_new = risk_keywords.append(risk_keywords_data)\n",
    "\n",
    "        with open('risk_text.pkl', 'wb') as f:\n",
    "            pickle.dump(risk_text_df_new, f)\n",
    "\n",
    "        with open('risk_keywords.pkl', 'wb') as f:\n",
    "            pickle.dump(risk_keywords_df_new, f)\n",
    "\n",
    "    except Exception as e:\n",
    "        print(\"Error in updating pickle file\")\n",
    "        print(e)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "id": "d6df2f8d",
   "metadata": {},
   "outputs": [],
   "source": [
    "#static risk extraction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "id": "533ec239",
   "metadata": {},
   "outputs": [],
   "source": [
    "def business_risk(company, year, clean_text):\n",
    "    #function to extract risk from clean text\n",
    "    business_risk = ' '\n",
    "    return {'Year': [year],company:[business_risk]}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "id": "09ad49c6",
   "metadata": {},
   "outputs": [],
   "source": [
    "def regulatory_risk(company, year, clean_text):\n",
    "    #function to extract risk from clean text\n",
    "    regulatory_risk = ' '\n",
    "    return {'Year': [year],company:[regulatory_risk]}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "id": "66742e69",
   "metadata": {},
   "outputs": [],
   "source": [
    "def aquisition_risk(company, year, clean_text):\n",
    "    #function to extract risk from clean text\n",
    "    aquisition_risk = ' '\n",
    "    return {'Year': [year],company:[aquisition_risk]}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "id": "4907ecf0",
   "metadata": {},
   "outputs": [],
   "source": [
    "def jurisdiction_risk(company, year, clean_text):\n",
    "    #function to extract risk from clean text\n",
    "    jurisdiction_risk = ' '\n",
    "    return {'Year': [year],company:[jurisdiction_risk]}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "id": "fecbd8fd",
   "metadata": {},
   "outputs": [],
   "source": [
    "def economic_risk(company, year, clean_text):\n",
    "    #function to extract risk from clean text\n",
    "    business_risk = ' '\n",
    "    return {'Year': [year],company:[business_risk]}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 131,
   "id": "281f2fad",
   "metadata": {},
   "outputs": [],
   "source": [
    "def update_static_risk(company,year,clean_text):\n",
    "    try:\n",
    "        with open('business_operational_risk.pkl', 'rb') as f:\n",
    "            business_risk_df = pickle.load(f)\n",
    "            print('business_risk')\n",
    "\n",
    "        with open('legal_regulatory_risk.pkl', 'rb') as f:\n",
    "            regulatory_risk_df = pickle.load(f)\n",
    "\n",
    "        with open('risk_related_aquisition.pkl', 'rb') as f:\n",
    "            aquisition_risk_df = pickle.load(f)\n",
    "\n",
    "        with open('risk_related_jurisdiction.pkl', 'rb') as f:\n",
    "            jurisdiction_risk_df = pickle.load(f)\n",
    "\n",
    "        with open('economic_industrial_risk.pkl', 'rb') as f:\n",
    "            economic_risk_df = pickle.load(f)\n",
    "    except Exception as e:\n",
    "        print(e)\n",
    "        print(\"Check pickle file is present or not for static risks\")\n",
    "    \n",
    "    business_risk_dict = business_risk(company, year, clean_text)\n",
    "    regulatory_risk_dict = regulatory_risk(company, year, clean_text)\n",
    "    aquisition_risk_dict = aquisition_risk(company, year, clean_text)\n",
    "    jurisdiction_risk_dict = jurisdiction_risk(company, year, clean_text)\n",
    "    economic_risk_dict = economic_risk(company, year, clean_text)\n",
    "    \n",
    "    \n",
    "    if business_risk_df['Year'].isin([year]).any():\n",
    "        print('date present')\n",
    "        print(business_risk_dict)\n",
    "        business_risk_df.loc[0,company] = business_risk_dict[company]\n",
    "        regulatory_risk_df.loc[0,company] = regulatory_risk_dict[company]\n",
    "        aquisition_risk_df.loc[0,company] = aquisition_risk_dict[company]\n",
    "        jurisdiction_risk_df.loc[0,company] = jurisdiction_risk_dict[company]\n",
    "        economic_risk_df.loc[0,company] = economic_risk_dict[company]\n",
    "\n",
    "    else:\n",
    "        print('create new row')\n",
    "        business_risk_dict_df = pd.DataFrame(business_risk_dict)\n",
    "        business_risk_df = business_risk_dict_df.append(business_risk_df)\n",
    "        print('regulatory_risk_dict')\n",
    "        print(regulatory_risk_dict)\n",
    "        regulatory_risk_dict_df = pd.DataFrame(regulatory_risk_dict)\n",
    "        regulatory_risk_df = regulatory_risk_dict_df.append(regulatory_risk_df)\n",
    "\n",
    "        aquisition_risk_dict_df = pd.DataFrame(aquisition_risk_dict)\n",
    "        aquisition_risk_df = aquisition_risk_dict_df.append(aquisition_risk_df)\n",
    "\n",
    "        jurisdiction_risk_dict_df = pd.DataFrame(jurisdiction_risk_dict)\n",
    "        jurisdiction_risk_df = jurisdiction_risk_dict_df.append(jurisdiction_risk_df)\n",
    "        \n",
    "        economic_risk_dict_df = pd.DataFrame(economic_risk_dict)\n",
    "        economic_risk_df = economic_risk_dict_df.append(economic_risk_df)\n",
    "        \n",
    "    with open('business_operational_risk.pkl', 'wb') as f:\n",
    "        pickle.dump(business_risk_df, f)\n",
    "    with open('legal_regulatory_risk.pkl', 'wb') as f:\n",
    "        pickle.dump(regulatory_risk_df, f)\n",
    "    with open('risk_related_aquisition.pkl', 'wb') as f:\n",
    "        pickle.dump(aquisition_risk_df, f)\n",
    "    with open('risk_related_jurisdiction.pkl', 'wb') as f:\n",
    "        pickle.dump(jurisdiction_risk_df, f)\n",
    "    with open('economic_industrial_risk.pkl', 'wb') as f:\n",
    "        pickle.dump(economic_risk_df, f)\n",
    "        \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 132,
   "id": "dfefd522",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Dynamic risk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 133,
   "id": "b46e6c25",
   "metadata": {},
   "outputs": [],
   "source": [
    "def patents_risks(company, year, clean_text):\n",
    "    #function to extract risk from clean text\n",
    "    patents_risks = ' '\n",
    "    return {'Year': [year],company:[patents_risks]}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 134,
   "id": "ff8cf346",
   "metadata": {},
   "outputs": [],
   "source": [
    "def RandD_expense_risks(company, year, clean_text):\n",
    "    #function to extract risk from clean text\n",
    "    randd_expense_risks = ' '\n",
    "    return {'Year': [year],company:[randd_expense_risks]}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 135,
   "id": "c4dc2e53",
   "metadata": {},
   "outputs": [],
   "source": [
    "def recall_risk(company, year, clean_text):\n",
    "    #function to extract risk from clean text\n",
    "    recall_risk = ' '\n",
    "    return {'Year': [year],company:[recall_risk]}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 136,
   "id": "d92a849d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def restructuring_cost_risks(company, year, clean_text):\n",
    "    #function to extract risk from clean text\n",
    "    restructuring_cost_risks = ' '\n",
    "    return {'Year': [year],company:[restructuring_cost_risks]}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 137,
   "id": "b181c719",
   "metadata": {},
   "outputs": [],
   "source": [
    "def acquisition_risks(company, year, clean_text):\n",
    "    #function to extract risk from clean text\n",
    "    acquisition_risks = ' '\n",
    "    return {'Year': [year],company:[acquisition_risks]}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 138,
   "id": "9913360a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def litigation_risks(company, year, clean_text):\n",
    "    #function to extract risk from clean text\n",
    "    litigation_risks = ' '\n",
    "    return {'Year': [year],company:[litigation_risks]}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 139,
   "id": "5b3280b4",
   "metadata": {},
   "outputs": [],
   "source": [
    "def new_patents_risks(company, year, clean_text):\n",
    "    #function to extract risk from clean text\n",
    "    new_patents_risks = ' '\n",
    "    return {'Year': [year],company:[new_patents_risks]}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 140,
   "id": "998a66ad",
   "metadata": {},
   "outputs": [],
   "source": [
    "def update_dynamic_risk(company,year,clean_text):\n",
    "    try:\n",
    "        with open('patents_risks.pkl', 'rb') as f:\n",
    "            patent_risk_df = pickle.load(f)\n",
    "            print('patent_risk_df')\n",
    "\n",
    "        with open('RandD_expense_risks.pkl', 'rb') as f:\n",
    "            RandD_expense_risks_df = pickle.load(f)\n",
    "\n",
    "        with open('recall_risk.pkl', 'rb') as f:\n",
    "            recall_risk_df = pickle.load(f)\n",
    "\n",
    "        with open('restructuring_cost_risks.pkl', 'rb') as f:\n",
    "            restructuring_cost_risks_df = pickle.load(f)\n",
    "\n",
    "        with open('acquisition_risks.pkl', 'rb') as f:\n",
    "            acquisition_risks_df = pickle.load(f)\n",
    "\n",
    "        with open('litigation_risks.pkl', 'rb') as f:\n",
    "            litigation_risks_df = pickle.load(f)\n",
    "            \n",
    "        with open('new_patents_risks.pkl', 'rb') as f:\n",
    "            new_patents_risks_df = pickle.load(f)\n",
    "            \n",
    "    except Exception as e:\n",
    "        print(e)\n",
    "        print(\"Check pickle file is present or not for static risks\")\n",
    "    \n",
    "    patents_risks_dict = patents_risks(company, year, clean_text)\n",
    "    RandD_expense_risks_dict = RandD_expense_risks(company, year, clean_text)\n",
    "    recall_risk_dict = recall_risk(company, year, clean_text)\n",
    "    restructuring_cost_risks_dict = restructuring_cost_risks(company, year, clean_text)\n",
    "    acquisition_risks_dict = acquisition_risks(company, year, clean_text)\n",
    "    litigation_risks_dict = litigation_risks(company, year, clean_text)\n",
    "    new_patents_risks_dict = new_patents_risks(company, year, clean_text)\n",
    "    \n",
    "    \n",
    "    if patents_risks_dict['YEAR'].isin([year]).any():\n",
    "        print('date present')\n",
    "        print(patents_risks_dict)\n",
    "        patent_risk_df.loc[0,company] = patents_risks_dict[company]\n",
    "        RandD_expense_risks_df.loc[0,company] = RandD_expense_risks_dict[company]\n",
    "        recall_risk_df.loc[0,company] = recall_risk_dict[company]\n",
    "        restructuring_cost_risks_df.loc[0,company] = restructuring_cost_risks_dict[company]\n",
    "        acquisition_risks_df.loc[0,company] = acquisition_risks_dict[company]\n",
    "        litigation_risks_df.loc[0,company] = litigation_risks_dict[company]\n",
    "        new_patents_risks_df.loc[0,company] = new_patents_risks_dict[company]\n",
    "\n",
    "    else:\n",
    "        print('create new row')\n",
    "        patents_risks_dict_df = pd.DataFrame(patents_risks_dict)\n",
    "        patent_risk_df = patents_risks_dict_df.append(patent_risk_df)\n",
    "        \n",
    "        RandD_expense_risks_dict_df = pd.DataFrame(RandD_expense_risks_dict)\n",
    "        RandD_expense_risks_df = RandD_expense_risks_dict_df.append(RandD_expense_risks_df)\n",
    "\n",
    "        recall_risk_dict_df = pd.DataFrame(recall_risk_dict)\n",
    "        recall_risk_df = recall_risk_dict_df.append(recall_risk_df)\n",
    "\n",
    "        restructuring_cost_risks_dict_df = pd.DataFrame(restructuring_cost_risks_dict)\n",
    "        restructuring_cost_risks_df = restructuring_cost_risks_dict_df.append(restructuring_cost_risks_df)\n",
    "        \n",
    "        acquisition_risks_dict_df = pd.DataFrame(acquisition_risks_dict)\n",
    "        acquisition_risks_df = acquisition_risks_dict_df.append(acquisition_risks_df)\n",
    "\n",
    "        litigation_risks_dict_df = pd.DataFrame(litigation_risks_dict)\n",
    "        litigation_risks_df = litigation_risks_dict_df.append(litigation_risks_df)\n",
    "\n",
    "        new_patents_risks_dict_df = pd.DataFrame(new_patents_risks_dict)\n",
    "        new_patents_risks_df = new_patents_risks_dict_df.append(new_patents_risks_df)\n",
    "\n",
    "    with open('patents_risks.pkl', 'wb') as f:\n",
    "        pickle.dump(patent_risk_df,f)\n",
    "        print('patent_risk_df')\n",
    "\n",
    "    with open('RandD_expense_risks.pkl', 'wb') as f:\n",
    "        pickle.dump(RandD_expense_risks_df,f)\n",
    "\n",
    "    with open('recall_risk.pkl', 'wb') as f:\n",
    "        pickle.dump(recall_risk_df,f)\n",
    "\n",
    "    with open('restructuring_cost_risks.pkl', 'wb') as f:\n",
    "        pickle.dump(restructuring_cost_risks_df,f)\n",
    "\n",
    "    with open('acquisition_risks.pkl', 'wb') as f:\n",
    "        pickle.dump(acquisition_risks_df,f)\n",
    "\n",
    "    with open('litigation_risks.pkl', 'wb') as f:\n",
    "        pickle.dump(litigation_risks_df,f)\n",
    "\n",
    "    with open('new_patents_risks.pkl', 'wb') as f:\n",
    "        pickle.dump(new_patents_risks_df,f)\n",
    "        \n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8af72ee2",
   "metadata": {},
   "source": [
    "##### scheduling job to run function of 10q updation everyday"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 142,
   "id": "36edfaee",
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "business_risk\n",
      "create new row\n",
      "regulatory_risk_dict\n",
      "{'Year': [2023], 'MDT': [' ']}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\deepali.b\\AppData\\Local\\Temp\\ipykernel_30768\\918765516.py:41: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.\n",
      "  business_risk_df = business_risk_dict_df.append(business_risk_df)\n",
      "C:\\Users\\deepali.b\\AppData\\Local\\Temp\\ipykernel_30768\\918765516.py:45: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.\n",
      "  regulatory_risk_df = regulatory_risk_dict_df.append(regulatory_risk_df)\n",
      "C:\\Users\\deepali.b\\AppData\\Local\\Temp\\ipykernel_30768\\918765516.py:48: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.\n",
      "  aquisition_risk_df = aquisition_risk_dict_df.append(aquisition_risk_df)\n",
      "C:\\Users\\deepali.b\\AppData\\Local\\Temp\\ipykernel_30768\\918765516.py:51: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.\n",
      "  jurisdiction_risk_df = jurisdiction_risk_dict_df.append(jurisdiction_risk_df)\n",
      "C:\\Users\\deepali.b\\AppData\\Local\\Temp\\ipykernel_30768\\918765516.py:54: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.\n",
      "  economic_risk_df = economic_risk_dict_df.append(economic_risk_df)\n"
     ]
    }
   ],
   "source": [
    "for company in company_ticker:\n",
    "    update_static_risk(company,year,' ')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8ba1ca4b",
   "metadata": {},
   "outputs": [],
   "source": [
    "for company in company_ticker:\n",
    "    update_dynamic_risk(company,year,' ')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 180,
   "id": "a56892bc",
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "company_name\n",
      "MDT\n",
      "company_ticker\n",
      "MDT\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\deepali.b\\Anaconda3\\envs\\tf\\lib\\site-packages\\bs4\\builder\\__init__.py:545: XMLParsedAsHTMLWarning: It looks like you're parsing an XML document using an HTML parser. If this really is an HTML document (maybe it's XHTML?), you can ignore or filter this warning. If it's XML, you should know that using an XML parser will be more reliable. To parse this document as XML, make sure you have the lxml package installed, and pass the keyword argument `features=\"xml\"` into the BeautifulSoup constructor.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "File downloaded successfully at given path\n",
      "os.path.join(path, name\n",
      "C:\\Users\\deepali.b\\DL_tensorflow\\10k_document\\10k_downlod_extraction/sec-edgar-filings\\MDT\\10-K\\0001613103-22-000023\\full-submission.txt\n",
      "path_list\n",
      "C:\\Users\\deepali.b\\DL_tensorflow\\10k_document\\10k_downlod_extraction/sec-edgar-filings\\MDT\\10-K\\0001613103-22-000023\\full-submission.txt\n",
      "Data Extraction is successfull!\n",
      "gross_df\n",
      "   YEAR  MDT (%)\n",
      "0  2022    67.98\n",
      "65.19%\n",
      "revenue_df\n",
      "   YEAR  MDT (%)\n",
      "0  2022     5.21\n",
      "5.21\n",
      "company_name_m\n",
      "MDT (M$)\n",
      "MDT (%)\n",
      "adding new row\n",
      "2022.0\n",
      "net_sales_update\n",
      "   Year\\USD millions  MDT (M$)\n",
      "0               2022     31686\n",
      "-----------------\n",
      "RangeIndex(start=0, stop=1, step=1)\n",
      "RangeIndex(start=0, stop=12, step=1)\n",
      "net_sales_df new row\n",
      "    Year\\USD millions MDT (M$) STE (M$) SYK (M$) JNJ (M$)  GMED (M$)\n",
      "0                2022    31686      NaN      NaN      NaN       <NA>\n",
      "1                2021   30,117    3,108   14,351   82,584        789\n",
      "2                2020   28,913    3,031   14,884   82,059        785\n",
      "3                2019   30,557    2,782   13,601   81,581        713\n",
      "4                2018   29,953    2,620   12,444   76,450        636\n",
      "5                2017   29,710    2,613   11,325   71,890        564\n",
      "6                2016   28,833      NaN    9,946   70,074        545\n",
      "7                2015   20,261      NaN    9,675   74,331        474\n",
      "8                2014   17,005      NaN    9,021   71,312        434\n",
      "9                2013   16,590      NaN    8,657      NaN        386\n",
      "10               2012      NaN      NaN    8,307      NaN        331\n",
      "11               2011      NaN      NaN    7,320      NaN        288\n",
      "12               2010      NaN      NaN      NaN      NaN       <NA>\n",
      "updating pickle file\n",
      "company_name\n",
      "STE\n",
      "company_ticker\n",
      "STE\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\deepali.b\\Anaconda3\\envs\\tf\\lib\\site-packages\\bs4\\builder\\__init__.py:545: XMLParsedAsHTMLWarning: It looks like you're parsing an XML document using an HTML parser. If this really is an HTML document (maybe it's XHTML?), you can ignore or filter this warning. If it's XML, you should know that using an XML parser will be more reliable. To parse this document as XML, make sure you have the lxml package installed, and pass the keyword argument `features=\"xml\"` into the BeautifulSoup constructor.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "File downloaded successfully at given path\n",
      "os.path.join(path, name\n",
      "C:\\Users\\deepali.b\\DL_tensorflow\\10k_document\\10k_downlod_extraction/sec-edgar-filings\\STE\\10-K\\0001757898-22-000011\\full-submission.txt\n",
      "path_list\n",
      "C:\\Users\\deepali.b\\DL_tensorflow\\10k_document\\10k_downlod_extraction/sec-edgar-filings\\STE\\10-K\\0001757898-22-000011\\full-submission.txt\n",
      "Data Extraction is successfull!\n",
      "gross_df\n",
      "   YEAR  STE (%)\n",
      "0  2022    43.97\n",
      "67.98\n",
      "revenue_df\n",
      "   YEAR  STE (%)\n",
      "0  2022    47.52\n",
      "47.52\n",
      "company_name_m\n",
      "STE (M$)\n",
      "STE (%)\n",
      "Index(['MDT (M$)', 'STE (M$)', 'SYK (M$)', 'JNJ (M$)', 'GMED (M$)'], dtype='object')\n",
      "updating existing row\n",
      "47.52\n",
      "   YEAR  STE (%)\n",
      "0  2022    47.52\n",
      "Index([2022], dtype='object')\n",
      "4585\n",
      "                  MDT (M$) STE (M$) SYK (M$) JNJ (M$)  GMED (M$)\n",
      "Year\\USD millions                                               \n",
      "2022                 31686     4585      NaN      NaN       <NA>\n",
      "2021                30,117    3,108   14,351   82,584        789\n",
      "2020                28,913    3,031   14,884   82,059        785\n",
      "2019                30,557    2,782   13,601   81,581        713\n",
      "2018                29,953    2,620   12,444   76,450        636\n",
      "2017                29,710    2,613   11,325   71,890        564\n",
      "2016                28,833      NaN    9,946   70,074        545\n",
      "2015                20,261      NaN    9,675   74,331        474\n",
      "2014                17,005      NaN    9,021   71,312        434\n",
      "2013                16,590      NaN    8,657      NaN        386\n",
      "2012                   NaN      NaN    8,307      NaN        331\n",
      "2011                   NaN      NaN    7,320      NaN        288\n",
      "2010                   NaN      NaN      NaN      NaN       <NA>\n",
      "updating pickle file\n",
      "company_name\n",
      "SYK\n",
      "company_ticker\n",
      "SYK\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\deepali.b\\Anaconda3\\envs\\tf\\lib\\site-packages\\bs4\\builder\\__init__.py:545: XMLParsedAsHTMLWarning: It looks like you're parsing an XML document using an HTML parser. If this really is an HTML document (maybe it's XHTML?), you can ignore or filter this warning. If it's XML, you should know that using an XML parser will be more reliable. To parse this document as XML, make sure you have the lxml package installed, and pass the keyword argument `features=\"xml\"` into the BeautifulSoup constructor.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "File downloaded successfully at given path\n",
      "os.path.join(path, name\n",
      "C:\\Users\\deepali.b\\DL_tensorflow\\10k_document\\10k_downlod_extraction/sec-edgar-filings\\SYK\\10-K\\0000310764-22-000028\\full-submission.txt\n",
      "path_list\n",
      "C:\\Users\\deepali.b\\DL_tensorflow\\10k_document\\10k_downlod_extraction/sec-edgar-filings\\SYK\\10-K\\0000310764-22-000028\\full-submission.txt\n",
      "Data Extraction is successfull!\n",
      "gross_df\n",
      "   YEAR  SYK (%)\n",
      "0  2022    64.11\n",
      "67.98\n",
      "revenue_df\n",
      "   YEAR  SYK (%)\n",
      "0  2022    19.21\n",
      "19.21\n",
      "company_name_m\n",
      "SYK (M$)\n",
      "SYK (%)\n",
      "Index(['MDT (M$)', 'STE (M$)', 'SYK (M$)', 'JNJ (M$)', 'GMED (M$)'], dtype='object')\n",
      "updating existing row\n",
      "19.21\n",
      "   YEAR  SYK (%)\n",
      "0  2022    19.21\n",
      "Index([2022], dtype='object')\n",
      "17108\n",
      "                  MDT (M$) STE (M$) SYK (M$) JNJ (M$)  GMED (M$)\n",
      "Year\\USD millions                                               \n",
      "2022                 31686     4585    17108      NaN       <NA>\n",
      "2021                30,117    3,108   14,351   82,584        789\n",
      "2020                28,913    3,031   14,884   82,059        785\n",
      "2019                30,557    2,782   13,601   81,581        713\n",
      "2018                29,953    2,620   12,444   76,450        636\n",
      "2017                29,710    2,613   11,325   71,890        564\n",
      "2016                28,833      NaN    9,946   70,074        545\n",
      "2015                20,261      NaN    9,675   74,331        474\n",
      "2014                17,005      NaN    9,021   71,312        434\n",
      "2013                16,590      NaN    8,657      NaN        386\n",
      "2012                   NaN      NaN    8,307      NaN        331\n",
      "2011                   NaN      NaN    7,320      NaN        288\n",
      "2010                   NaN      NaN      NaN      NaN       <NA>\n",
      "updating pickle file\n",
      "company_name\n",
      "JNJ\n",
      "company_ticker\n",
      "JNJ\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\deepali.b\\Anaconda3\\envs\\tf\\lib\\site-packages\\bs4\\builder\\__init__.py:545: XMLParsedAsHTMLWarning: It looks like you're parsing an XML document using an HTML parser. If this really is an HTML document (maybe it's XHTML?), you can ignore or filter this warning. If it's XML, you should know that using an XML parser will be more reliable. To parse this document as XML, make sure you have the lxml package installed, and pass the keyword argument `features=\"xml\"` into the BeautifulSoup constructor.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "File downloaded successfully at given path\n",
      "os.path.join(path, name\n",
      "C:\\Users\\deepali.b\\DL_tensorflow\\10k_document\\10k_downlod_extraction/sec-edgar-filings\\JNJ\\10-K\\0000200406-22-000022\\full-submission.txt\n",
      "path_list\n",
      "C:\\Users\\deepali.b\\DL_tensorflow\\10k_document\\10k_downlod_extraction/sec-edgar-filings\\JNJ\\10-K\\0000200406-22-000022\\full-submission.txt\n",
      "Data Extraction is successfull!\n",
      "gross_df\n",
      "   YEAR  JNJ (%)\n",
      "0  2022    68.16\n",
      "67.98\n",
      "revenue_df\n",
      "   YEAR  JNJ (%)\n",
      "0  2022    13.55\n",
      "13.55\n",
      "company_name_m\n",
      "JNJ (M$)\n",
      "JNJ (%)\n",
      "Index(['MDT (M$)', 'STE (M$)', 'SYK (M$)', 'JNJ (M$)', 'GMED (M$)'], dtype='object')\n",
      "updating existing row\n",
      "13.55\n",
      "   YEAR  JNJ (%)\n",
      "0  2022    13.55\n",
      "Index([2022], dtype='object')\n",
      "93775\n",
      "                  MDT (M$) STE (M$) SYK (M$) JNJ (M$)  GMED (M$)\n",
      "Year\\USD millions                                               \n",
      "2022                 31686     4585    17108    93775       <NA>\n",
      "2021                30,117    3,108   14,351   82,584        789\n",
      "2020                28,913    3,031   14,884   82,059        785\n",
      "2019                30,557    2,782   13,601   81,581        713\n",
      "2018                29,953    2,620   12,444   76,450        636\n",
      "2017                29,710    2,613   11,325   71,890        564\n",
      "2016                28,833      NaN    9,946   70,074        545\n",
      "2015                20,261      NaN    9,675   74,331        474\n",
      "2014                17,005      NaN    9,021   71,312        434\n",
      "2013                16,590      NaN    8,657      NaN        386\n",
      "2012                   NaN      NaN    8,307      NaN        331\n",
      "2011                   NaN      NaN    7,320      NaN        288\n",
      "2010                   NaN      NaN      NaN      NaN       <NA>\n",
      "updating pickle file\n",
      "company_name\n",
      "GMED\n",
      "company_ticker\n",
      "GMED\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\deepali.b\\Anaconda3\\envs\\tf\\lib\\site-packages\\bs4\\builder\\__init__.py:545: XMLParsedAsHTMLWarning: It looks like you're parsing an XML document using an HTML parser. If this really is an HTML document (maybe it's XHTML?), you can ignore or filter this warning. If it's XML, you should know that using an XML parser will be more reliable. To parse this document as XML, make sure you have the lxml package installed, and pass the keyword argument `features=\"xml\"` into the BeautifulSoup constructor.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "File downloaded successfully at given path\n",
      "os.path.join(path, name\n",
      "C:\\Users\\deepali.b\\DL_tensorflow\\10k_document\\10k_downlod_extraction/sec-edgar-filings\\GMED\\10-K\\0001562762-22-000037\\full-submission.txt\n",
      "path_list\n",
      "C:\\Users\\deepali.b\\DL_tensorflow\\10k_document\\10k_downlod_extraction/sec-edgar-filings\\GMED\\10-K\\0001562762-22-000037\\full-submission.txt\n",
      "Data Extraction is successfull!\n",
      "gross_df\n",
      "   YEAR  GMED (%)\n",
      "0  2022     75.05\n",
      "67.98\n",
      "revenue_df\n",
      "   YEAR  GMED (%)\n",
      "0  2022     21.42\n",
      "21.42\n",
      "company_name_m\n",
      "GMED (M$)\n",
      "GMED (%)\n",
      "Index(['MDT (M$)', 'STE (M$)', 'SYK (M$)', 'JNJ (M$)', 'GMED (M$)'], dtype='object')\n",
      "updating existing row\n",
      "21.42\n",
      "   YEAR  GMED (%)\n",
      "0  2022     21.42\n",
      "Index([2022], dtype='object')\n",
      "958\n",
      "                  MDT (M$) STE (M$) SYK (M$) JNJ (M$)  GMED (M$)\n",
      "Year\\USD millions                                               \n",
      "2022                 31686     4585    17108    93775        958\n",
      "2021                30,117    3,108   14,351   82,584        789\n",
      "2020                28,913    3,031   14,884   82,059        785\n",
      "2019                30,557    2,782   13,601   81,581        713\n",
      "2018                29,953    2,620   12,444   76,450        636\n",
      "2017                29,710    2,613   11,325   71,890        564\n",
      "2016                28,833      NaN    9,946   70,074        545\n",
      "2015                20,261      NaN    9,675   74,331        474\n",
      "2014                17,005      NaN    9,021   71,312        434\n",
      "2013                16,590      NaN    8,657      NaN        386\n",
      "2012                   NaN      NaN    8,307      NaN        331\n",
      "2011                   NaN      NaN    7,320      NaN        288\n",
      "2010                   NaN      NaN      NaN      NaN       <NA>\n",
      "updating pickle file\n"
     ]
    }
   ],
   "source": [
    "#company_list = ['MDT','STE','SYK','JNJ','GMED']\n",
    "for company in company_ticker:\n",
    "    update_10_k(year,company,form_type_k)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4a7ce34d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# update 10_q"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4c24eb14",
   "metadata": {},
   "outputs": [],
   "source": [
    "update_10_q()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a052009b",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# scheduler = BlockingScheduler()\n",
    "# scheduler.add_job(func=update_10_q, trigger='interval', hours=24, id='10_q updation')\n",
    "# scheduler.start()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "092b45c8",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
